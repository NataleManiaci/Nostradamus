{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Tesla with Machine Learning and Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project we will aim to use a Machine Learning model to predict the daily prices of the Tesla stock. Along with several technical indicators derived from the OHLCV data, we will feed into the model also the sentiment scores of relevant Twitter posts, thus leveraging Sentiment Analysis, a Natural Language Processing tecnique. All these data will be used to train the model which in turn will predict the trading signals.\n",
    "\n",
    "Sentiment analysis and generally the positive or negative sentiment on news about a stock has been studied in depth in the recent years, finding a strong correlation between the sentiment score of such data and the stock price movement <sup><a href='#References'>[1]</a></sup>.\n",
    "\n",
    "The labels we need to predict are the buy and sell signals, which we will assign based on simple price conditions in order to train the model. Since these are discrete data, the problem we are trying to solve is a supervised classification.\n",
    "\n",
    "The Machine Learning model we wil use is the Random Forest Classifier. Random Forests are supervised algorithms consisting in multiple decision trees performing well with non-linear data and with low risk of overfitting. They are also strong against outliers, although they suffer of low interpretability compared to single decison trees <sup><a href='#References'>[2]</a></sup>.\n",
    "\n",
    "The reliability of the Random Forest in predicting stock prices is widely confirmed by many studies, suggesting the robustness of this model when compared to other machine learning algorithms. The paper \"Predicting Stock Market Price Direction with Uncertainty Using Quantile Regression Forest\" <sup><a href='#References'>[3]</a></sup> raises an observation regarding the uncertainty of predictions of the Random Forest model, stating that it's scarcely studied subject. This is surely an aspect worth to investigate for researchers, given the financial risk involved with applications such as algorhitmic trading.\n",
    "\n",
    "In this project we will also leverage techniques and paradigms such as concurrency, functional and object oriented programming, idempotence, Direct Acyclic Graph and data pipeline.\n",
    "\n",
    "The intention is to design a program with a sleek interface and with a level of automation such that the user is able to tailor the details and output of the program by entering a minimal amount of data, partly even in an interactive way. This should make the project reusable, meaning that it's easy to carry out the backtest of the trading strategy on another asset. Furthermore, the modularity of the software design should facilitate changes to adatpt the program to different requirements (i.e. different data or ML models).\n",
    "\n",
    "Please be aware that the content and results of this project do not represent financial advise. You should conduct your own research before trading or investing in the markets. Your capital is at risk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import yfinance as yf\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import csv\n",
    "from datetime import datetime, date, timedelta\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import copy\n",
    "import talib as ta\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import multiprocessing as mp\n",
    "import threading\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler\n",
    "from sklearn import metrics\n",
    "from pprint import pprint\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data download and manipulation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Directed Acyclic Graph (DAG) wrapped into a pipeline class to manage the Twitter data and stock prices download and manipulation. The pipeline will manage the data folders creation, the data download operations as well as the validation of the input parameters required by the user so that the aggregated data for the machine learning model will update automatically based on the initial parameters passed. This will make the project reusable and will leave a friendly interface to the user, who will be required to enter the parameters once at the beginning before running the pipeline.\n",
    "\n",
    "A Directed Acyclic Graph is a set of nodes with a direction and where it's not possible to find a cyclic path <sup><a href='#References'>[4]</a></sup>. A DAG goes beyond a simple data structure, it's versatile and in fact is used in a wide range of applications, from project management to distributed ledgers technology such as blockchain. \n",
    "\n",
    "I built the DAG used in this project whilst studying and completing the Data Engineer course taken on the online learning platform Dataquest <sup><a href='#References'>[5]</a></sup> some time ago."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the directed acyclic graph class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DAG:\n",
    "    \"\"\"Directed Acyclic Graph.\n",
    "    \n",
    "    Attributes:\n",
    "        graph: dict type, nodes container\n",
    "    \n",
    "    Methods:\n",
    "        in_degrees(): number of nodes dependencies\n",
    "        sort(): order nodes in increasing dependencies\n",
    "        add(node, to=None): add node and pointed (if passed) to graph \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.graph = {}\n",
    "\n",
    "    def in_degrees(self):\n",
    "        \"\"\"Build and return dict of pairs node : number of pointers to node.\n",
    "        \"\"\"\n",
    "        \n",
    "        in_degrees = {}\n",
    "        for node in self.graph:\n",
    "            if node not in in_degrees:\n",
    "                in_degrees[node] = 0\n",
    "            for pointed in self.graph[node]:\n",
    "                if pointed not in in_degrees:\n",
    "                    in_degrees[pointed] = 0\n",
    "                in_degrees[pointed] += 1\n",
    "        return in_degrees\n",
    "\n",
    "    def sort(self):\n",
    "        \"\"\"Return sorted list of nodes.\n",
    "        Sorting from roots to most pointed nodes.\n",
    "        \"\"\"\n",
    "        \n",
    "        in_degrees = self.in_degrees()\n",
    "        to_visit = deque()\n",
    "        for node in self.graph:\n",
    "            if in_degrees[node] == 0:\n",
    "                to_visit.append(node)\n",
    "\n",
    "        searched = []\n",
    "        while to_visit:\n",
    "            node = to_visit.popleft()\n",
    "            for pointer in self.graph[node]:\n",
    "                in_degrees[pointer] -= 1\n",
    "                if in_degrees[pointer] == 0:\n",
    "                    to_visit.append(pointer)\n",
    "            searched.append(node)\n",
    "        return searched\n",
    "\n",
    "    def add(self, node, to=None):\n",
    "        \"\"\" Add node and pointed node.\n",
    "        If pointed node is not in graph, add it.\n",
    "            \n",
    "        Arguments:\n",
    "            node: function\n",
    "            to: function, takes node output as input\n",
    "        \"\"\"\n",
    "        if node not in self.graph:\n",
    "            self.graph[node] = []\n",
    "        if to:\n",
    "            if to not in self.graph:\n",
    "                self.graph[to] = []\n",
    "            self.graph[node].append(to)\n",
    "        if len(self.sort()) != len(self.graph):\n",
    "            raise Exception"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the pipeline class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline:\n",
    "    \"\"\"Wrapper of DAG data structure. Interface to add\n",
    "    and run tasks.\n",
    "    \n",
    "    Methods:\n",
    "        task(): wrapper for DAG's add method, pass task to DAG\n",
    "        run(): run pipeline tasks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tasks = DAG()\n",
    "\n",
    "    def task(self, depends_on=None):\n",
    "        \"\"\"Add task and its dependency (if passed)\n",
    "        \n",
    "        Arguments:\n",
    "            depends_on: function whose output is input for given task\n",
    "        \"\"\"\n",
    "        \n",
    "        def inner(f):\n",
    "            self.tasks.add(f)\n",
    "            if depends_on:\n",
    "                self.tasks.add(depends_on, f)\n",
    "            return f\n",
    "        return inner\n",
    "\n",
    "    def run(self):\n",
    "        \"\"\"Run tasks and return dict of pairs task : output\n",
    "        \"\"\"\n",
    "        \n",
    "        scheduled = self.tasks.sort()\n",
    "        completed = {}\n",
    "\n",
    "        for task in scheduled:\n",
    "            for node, values in self.tasks.graph.items():\n",
    "                if task in values:\n",
    "                    completed[task] = task(completed[node])\n",
    "            if task not in completed:\n",
    "                completed[task] = task()\n",
    "        return completed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate a Pipeline instance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now instantiate an instance of the pipeline, so that we can push the functions which will manage the data download and manipulation inside the D.A.G. pipeline object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " TSLA_data_pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using multithreading and multiprocess computing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we will be dealing with multiple data files to download and process, and long computations such as the sentiment scores calculations on the Twitter posts, we would benefit from using multiprocess and multithread computation. During the project development I have identified few tasks that represented a bottleneck for the execution's speed due to the process running serially. Thus, we will implement parallelization as follow:\n",
    "<br>\n",
    "<br>\n",
    "* tweets and prices data download: multithread\n",
    "* tweets aggregation: multiprocess\n",
    "* tweets sentiment score: multiprocess\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The above solution is consistent with the characteristics of these two techniques: the download is an i/o bound process and so it benefits from multiple threads, whereas the tweets aggregation and the computation of the sentiment scores on a large amount of data are CPU bound computations, therefore it can benefit from exploiting all the cores available.\n",
    "\n",
    "Let's define then two classes for handling multiprocess and multithread tasks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multiprocess_Executor():\n",
    "    \"\"\"\n",
    "    Wrapper class for multiprocess executor.\n",
    "        \n",
    "    Attributes:\n",
    "        path = directory, path to files to process\n",
    "        make_tasks_pool = function, must take path as input and return iterable of tasks\n",
    "        tasks_pool = iterable, pool of tasks to be executed by worker\n",
    "        worker: function, must take single task as input\n",
    "        handle_outputs_pool = function, must take iterable of outputs as input\n",
    "        n_cores = int, numbers of cores detected\n",
    "        \n",
    "    Methods:\n",
    "        make_tasks_pool: wrapper for make_tasks_pool function\n",
    "        handle_outputs_pool: wrapper for handle_outputs_pool function\n",
    "        execute: public, class caller to run all from reading in path to handling outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, to_do, make_tasks_pool, worker, handle_outputs_pool):\n",
    "        self.to_do = to_do\n",
    "        self.make_tasks_pool = make_tasks_pool\n",
    "        self.tasks_pool = self.make_tasks_pool(to_do)\n",
    "        self.worker = worker\n",
    "        self.handle_outputs_pool = handle_outputs_pool\n",
    "        self.n_cores = mp.cpu_count()\n",
    "    \n",
    "    def make_tasks_pool(self):\n",
    "        return self.make_tasks_pool(self.to_do)\n",
    "    \n",
    "    def handle_outputs_pool(self, outputs_pool):\n",
    "        return self.handle_outputs_pool(outputs_pool)\n",
    "        \n",
    "    def execute(self):\n",
    "        processes = ProcessPoolExecutor(max_workers=self.n_cores, mp_context=mp.get_context('fork'))\n",
    "        outputs_pool = list(processes.map(self.worker, self.tasks_pool))\n",
    "        return self.handle_outputs_pool(outputs_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Multithread_Executor():\n",
    "    \"\"\"\n",
    "    Wrapper class for multithread executor.\n",
    "        \n",
    "    Attributes:\n",
    "        path = directory, path to files to process\n",
    "        make_tasks_pool = function, must take path as input and return iterable of tasks\n",
    "        tasks_pool = iterable, pool of tasks to be executed by worker\n",
    "        worker: function, must take single task as input\n",
    "        handle_outputs_pool = function, must take iterable of outputs as input\n",
    "        n_threads = int, initialised as number of tasks in the pool\n",
    "        \n",
    "    Methods:\n",
    "        make_tasks_pool: wrapper for make_tasks_pool function\n",
    "        handle_outputs_pool: wrapper for handle_outputs_pool function\n",
    "        execute: public, class caller to run all from reading in path to handling outputs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, to_do, make_tasks_pool, worker, handle_outputs_pool):\n",
    "        self.to_do = to_do\n",
    "        self.make_tasks_pool = make_tasks_pool\n",
    "        self.tasks_pool = self.make_tasks_pool(to_do)\n",
    "        self.worker = worker\n",
    "        self.handle_outputs_pool = handle_outputs_pool\n",
    "        self.n_threads = min(len(self.tasks_pool), 4)\n",
    "    \n",
    "    def make_tasks_pool(self):\n",
    "        return self.make_tasks_pool(self.path)\n",
    "    \n",
    "    def handle_outputs_pool(self, outputs_pool):\n",
    "        return self.handle_outputs_pool(outputs_pool)\n",
    "        \n",
    "    def execute(self):\n",
    "        threads = ThreadPoolExecutor(max_workers=self.n_threads)\n",
    "        outputs_pool = list(threads.map(self.worker, self.tasks_pool))\n",
    "        return self.handle_outputs_pool(outputs_pool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Multiprocess_Executor and Multithread_Executor classes represent a general interface for the parallel computing workflow in Python as implemented into the Executor class from concurrent.futures.\n",
    "\n",
    "We can now proceed to define the specific functions passed into the class, as needed for each task we'll need to carry out."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The data download and manipulation functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now proceed to write the functions which will handle the data (historical prices and Twitter posts) download and manipulation. These functions will be pushed into the Pipeline class object. This serves as wrapper and organizer (through the DAG structure) for the functions, which will be executed in order and each function output will be fed as input to the next function in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manage the data folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task we want to perform is managing the root directory where the downloaded files will be saved. We will have a \"data\" folder as root and the first subdirectory will be named as the ticker of the stock on which we will be running the analysis and ML model, in this case TESLA, which is listed as \"TSLA\".\n",
    "&nbsp;\n",
    "\n",
    "&nbsp;\n",
    "\n",
    "The \"data\" directory will have the following structure:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "raw_mimetype": "text/markdown"
   },
   "source": [
    "data\n",
    "  └── prices\n",
    "  └── tweets_raw\n",
    "  └── tweets_unstacked\n",
    "  └── tweets_merged\n",
    "  └── tweets_sentiment_scores\n",
    "  └── prices_TI_sentiment_scores\n",
    "  └── strategy_performance\n",
    "  └── model_training_parameters\n",
    "                               └── best_params\n",
    "                               └── param_grids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write a function that will initiate the pipeline by cheking if the \"data\" folder has been already created, and if not, it creates it. The function will simply return the \"data\" folder path so that the next function in the pipeline can take the job from there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task()\n",
    "def manage_data_directory():\n",
    "    \"\"\"\n",
    "    Check if the root directory for storing the project data exists.\n",
    "    Create it if it doesn't exist.\n",
    "    Return directory name.\n",
    "    \"\"\"\n",
    "    \n",
    "    root = 'data/'\n",
    "    if os.path.isdir(root):\n",
    "        print('The \"data\" directory already exists. Now working on prices and tweets download:')\n",
    "    else:\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        print('\"data\" directory created. Now working on prices and tweets download:\\n')\n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The scope of the pipeline is to abstract the user from the download and manipulation of data, leaving to her/him only the task to set the download parameters such as the dates of the period on which to run the analysis, the Twitter accounts where to retrieve the posts and some preferences as to how to perform the tweets search.\n",
    "\n",
    "Let's define a data structure where the user will input these parameters. We will use a dictionary as it allows us to have a nested mapping of the key parameters needed for the data dwonload.\n",
    "\n",
    "The dictionary's structure is represented below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "download_params = {'ticker' : 'TSLA',\n",
    "                   'since' : '2010-06-29', \n",
    "                   'until' : '2021-06-02',\n",
    "                   'twitter_scrape_by_account' : {'elonmusk': {'search_keyword' : '',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'tesla': {'search_keyword' : '',\n",
    "                                                            'by_hashtag' : False},\n",
    "                                                  'WSJ' : {'search_keyword' : 'Tesla',\n",
    "                                                           'by_hashtag' : False},\n",
    "                                                  'Reuters' : {'search_keyword' : 'Tesla',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'business': {'search_keyword' : 'Tesla',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'CNBC': {'search_keyword' : 'Tesla',\n",
    "                                                           'by_hashtag' : False},\n",
    "                                                  'FinancialTimes' : {'search_keyword' : 'Tesla',\n",
    "                                                                      'by_hashtag' : True}},\n",
    "                   'twitter_scrape_by_most_popular' : {'all_twitter_1': {'search_keyword' : 'Tesla',\n",
    "                                                                       'max_tweets_per_day' : 30,\n",
    "                                                                       'by_hashtag' : True}},\n",
    "                   'language' : 'en'                                      \n",
    "                  }\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The meaning and use of the keys in the dictionary is as follows:\n",
    "<br>\n",
    "<br>\n",
    "* \"ticker\": the symbol of the stock\n",
    "* \"since\": the start date for historical prices and tweets\n",
    "* \"until\": the end date for historical prices and tweets\n",
    "* \"language\": the code representing the language for the tweets (if in doubt, consult the Twitter API)\n",
    "* \"twitter_scrape_by_account\": details to download tweets from a single Twitter account serching by a keyword\n",
    "    * the key in each entry (i.e \"elonmusk\", \"Reuters\" in the instance above) must be the username (handle) of the Twitter account\n",
    "    * \"search_keyword\" is the keyword to look for in the tweets\n",
    "    * if the \"search_keyword\" value is an empty string, all the tweets posted by the account will be downloaded\n",
    "    * \"by_hashtag\" is a boolean to define whether to use the keyword as hashtag or simple text\n",
    "* \"twitter_scrape_by_most_popular\": details to download tweets from any Twitter accounts searching by a keyword and ranked by most retweeted\n",
    "    * the key (i.e. \"all_twitter_1\" in the instance above) can be anything as it's not used for file naming purposes. If multiple entries in \"twitter_scrape_by_most_popular\", make sure these keys are different\n",
    "    * \"search_keyword\" is the keyword to look for in the tweets. Ideally a keyword should be passed, otherwise it will try to download any tweets posted on Twitter between the provided timeframe and this may slow down or make the download fail\n",
    "    * \"max_tweets_per_day\": the maximum number of tweets to select for each day. They will be ranked by number of retweet after all tweets are scraped\n",
    "    * \"by_hashtag\" is a boolean to define whether to use the keyword as hashtag or simple text\n",
    "\n",
    "<br>\n",
    "Few rules to keep in mind whilst filling up the download parameters dictionary:\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "* All \"ticker\", \"since\", \"until\", \"language\", \"twitter_scrape_by_account\", \"twitter_scrape_by_most_popular\" key/value pairs are required\n",
    "* If there are no entries for \"twitter_scrape_by_account\" or \"twitter_scrape_by_most_popular\", just enter as value an empty list [], tuple () or string \"\" instead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use of Idempotence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The intention behind the pipeline, as mentioned earlier, is to abstract the download and manipulation of data from the user. This means that we want to handle any variations the user wants to bring in sourcing the data (mostly tweets, but also dates of the period of the analysis and historical prices consequently).\n",
    "\n",
    "To do so, we have to keep a log of the status for the content of data, prices and tweets, in the root folder. Thus, if the user changes any of the download parameters, the pipeline will handle that change and download only what has been added and delete what has been suppressed from the parameters. This will give complete control on the files downloaded and manipulated by just altering the download parameters dictionary.\n",
    "\n",
    "We will achieve this by naming the files based on all the parameters, so that there is an unanbiguous relation between download parameters and file names, which allows us to scan the data folder and let the pipeline execute the job needed to update all the data based on the current download parameters.\n",
    "\n",
    "This concept is known as idempotence, the property of a function to give the same output any times it is called with the same input <sup><a href='#References'>[6]</a></sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Historical prices download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's write first a function to handle the historical prices download:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=manage_data_directory)\n",
    "def download_prices(root):\n",
    "    \"\"\"\n",
    "    Scan or create prices folder in root data directory.\n",
    "    Download relevant historical prices (as per download_params).\n",
    "    Delete non relevant existing prices file.\n",
    "    Return prices folder path for next prices' manipulation function in the pipeline.\n",
    "    \n",
    "    Arguments:\n",
    "        root: string, project's data root directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build prices file name\n",
    "    ticker = download_params['ticker'] + '_'\n",
    "    since = download_params['since'].replace('-','') + '_'\n",
    "    until = download_params['until'].replace('-','') + '_'\n",
    "    prices_filename = ticker + since + until + 'prices.csv'\n",
    "    \n",
    "    # If prices directory does not exist\n",
    "    prices_folder = root + 'prices/'\n",
    "    if not os.path.exists(prices_folder):\n",
    "        # create it\n",
    "        os.makedirs(prices_folder)\n",
    "        # download and save prices with file name\n",
    "        ticker = download_params['ticker']\n",
    "        since = download_params['since']\n",
    "        until = download_params['until']\n",
    "        until_date = datetime.strptime(until, '%Y-%m-%d').date()\n",
    "        until_incl = datetime.strftime(until_date + timedelta(1), '%Y-%m-%d')\n",
    "        print('\"prices\" folder created. Downloading historical prices for', ticker + '...')\n",
    "        df = yf.download(ticker, since, until_incl)\n",
    "        prices_path = prices_folder + prices_filename\n",
    "        df.to_csv(prices_path)\n",
    "        print(ticker, 'prices downloaded in \"prices\" folder and saved as', prices_filename + '.')\n",
    "        \n",
    "    # if prices directory exists, check if relevant prices file already exists or download and save it\n",
    "    else:\n",
    "        print('Scanning the \"prices\" folder...')\n",
    "        existing_files = [file.split('/')[2] for file in glob.glob(prices_folder + '*')]\n",
    "        # Check also prices with technical indicators\n",
    "        prices_filename_TI = prices_filename.replace('.csv', '_TI.csv')\n",
    "        to_delete_files = [file for file in existing_files if (file != prices_filename and file != prices_filename_TI)]\n",
    "        if (prices_filename not in existing_files) and (prices_filename_TI not in existing_files):\n",
    "            # download and save prices with file name\n",
    "            ticker = download_params['ticker']\n",
    "            since = download_params['since']\n",
    "            until = download_params['until']\n",
    "            until_date = datetime.strptime(until, '%Y-%m-%d').date()\n",
    "            until_incl = datetime.strftime(until_date + timedelta(1), '%Y-%m-%d')\n",
    "            df = yf.download(ticker, since, until_incl)\n",
    "            prices_path = prices_folder + prices_filename\n",
    "            df.to_csv(prices_path)\n",
    "            print(ticker, 'prices downloaded in \"prices\" folder and saved as', prices_filename + '.')\n",
    "        \n",
    "        else:\n",
    "            ticker = download_params['ticker']\n",
    "            print('Relevant', ticker, 'prices file already exists as', prices_filename + '.')\n",
    "        \n",
    "        # if irrelevant prices files in the directory, delete them\n",
    "        if to_delete_files:\n",
    "            for file in to_delete_files:\n",
    "                file = prices_folder + file\n",
    "                os.remove(file)\n",
    "            print('Irrelevant existing prices files deleted.')\n",
    "        \n",
    "    # Return prices directory for next prices' manipulation function in the pipeline\n",
    "    return prices_folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The technical indicators"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next in the chain and depending on the \"download_prices\" function will be another functionto add technical indicators to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=download_prices)\n",
    "def technical_indicators(prices_folder):\n",
    "    \"\"\"\n",
    "    Add technical indicators to relevant prices file.\n",
    "    Return prices folder path\n",
    "    \n",
    "    Arguments:\n",
    "        prices_folder: str, prices data folder path  \n",
    "    \"\"\"\n",
    "    \n",
    "    # Check if prices file with technical indicators exists and if not build and save it \n",
    "    prices_path = glob.glob(prices_folder + '*')\n",
    "    if prices_path:\n",
    "        prices_path = prices_path[0]\n",
    "        \n",
    "    if '_TI' not in prices_path:\n",
    "        print(\"\\nComputing technical indicators on historical prices...\")\n",
    "        \n",
    "        # Load prices in pandas dataframe\n",
    "        df = pd.read_csv(prices_path, index_col=[0])\n",
    "        \n",
    "        # Prepare columns for TA-lib\n",
    "        Open = df['Open'].values \n",
    "        close = df['Close'].values\n",
    "        high = df['High'].values\n",
    "        low = df['Low'].values\n",
    "        volume = df['Volume'].astype('float64').values\n",
    "        \n",
    "        # TA-Lib Overlap indicators\n",
    "        df['BB_Up'], df['BB_Mid'], df['BB_Low'] = ta.BBANDS(close, timeperiod=20)\n",
    "        df['DEMA'] = ta.DEMA(close)\n",
    "        df['EMA_30'] = ta.EMA(close)\n",
    "        df['HT_Trendline'] = ta.HT_TRENDLINE(close)\n",
    "        df['KAMA'] = ta.KAMA(close)\n",
    "        df['MA_50'] = ta.MA(close, timeperiod=50)\n",
    "        df['MA_200'] = ta.MA(close, timeperiod=200)\n",
    "        df['MAMA'], df['FAMA'] = ta.MAMA(close)\n",
    "        df['Midpoint_14'] = ta.MIDPOINT(close)\n",
    "        df['Midprice_14'] = ta.MIDPRICE(high, low)\n",
    "        df['SAR'] = ta.SAR(high, low)\n",
    "        #df['SAREXT'] = ta.SAREXT(high, low)\n",
    "        df['T3_5'] = ta.T3(close)\n",
    "        df['TEMA_30'] = ta.TEMA(close)\n",
    "        df['TRIMA_30'] = ta.TRIMA(close)\n",
    "        df['WMA_30'] = ta.WMA(close)\n",
    "        \n",
    "        # Ta-Lib Momentum indicators\n",
    "        df['ADX_14'] = ta.ADX(high, low, close)\n",
    "        df['ADXR_14'] = ta.ADXR(high, low, close)\n",
    "        df['APO'] = ta.APO(close)\n",
    "        df['Aroon_Down_14'], df['Aroon_Up_14'] = ta.AROON(high, low)\n",
    "        df['Aroonosc_14'] = ta.AROONOSC(high, low)\n",
    "        df['BOP'] = ta.BOP(Open, high, low, close)\n",
    "        df['CCI_14'] = ta.CCI(high, low, close)\n",
    "        df['CMO_14'] = ta.CMO(close)\n",
    "        df['DX_14'] = ta.DX(high, low, close)\n",
    "        df['MACD_12_26'], df['MACD_signal'], df['MACDhist'] = ta.MACD(close)\n",
    "        df['MFI_14'] = ta.MFI(high, low, close, volume)\n",
    "        df['Minus_DI_14'] = ta.MINUS_DI(high, low, close)\n",
    "        df['Minus_DM_14'] = ta.MINUS_DM(high, low)\n",
    "        df['MOM_14'] = ta.MOM(close)\n",
    "        df['Plus_DI_14'] = ta.PLUS_DI(high, low, close)\n",
    "        df['Plus_DM_14'] = ta.PLUS_DM(high, low)\n",
    "        df['PPO_12_26'] = ta.PPO(close)\n",
    "        df['ROC_10'] = ta.ROC(close)\n",
    "        df['ROCP_10'] = ta.ROCP(close)\n",
    "        df['ROCR_10'] = ta.ROCR(close)\n",
    "        df['ROCR100_10'] =ta.ROCR100(close)\n",
    "        df['RSI_14'] = ta.RSI(close)\n",
    "        df['Stoch_Slowk_5_3'], df['Stoch_Slowd_3'] = ta.STOCH(high, low, close)\n",
    "        df['Stoch_Fastk_5'], df['Stoch_Fastd_3'] = ta.STOCHF(high, low, close)\n",
    "        df['StochRSI__14_Fastk_5'], df['StochRSI_14_Fastd_3'] = ta.STOCHRSI(close)\n",
    "        df['TRIX_30'] = ta.TRIX(close)\n",
    "        df['ULTOSC_7_14_28'] = ta.ULTOSC(high, low, close)\n",
    "        df['WILLR_14'] = ta.WILLR(high, low, close)\n",
    "        \n",
    "        # Ta-lib Volume indicators\n",
    "        df['AD'] = ta.AD(high, low, close, volume)\n",
    "        df['ADOSC_3_10'] = ta.ADOSC(high, low, close, volume)\n",
    "        df['OBV'] = ta.OBV(close, volume)\n",
    "        \n",
    "        # Ta-Lib Cycle indicators\n",
    "        df['DCPeriod'] = ta.HT_DCPERIOD(close)\n",
    "        df['DCPhasse'] = ta.HT_DCPHASE(close)\n",
    "        df['InPhase'], df['Quadrature'] = ta.HT_PHASOR(close)\n",
    "        df['Sine'], df['LeadSine'] = ta.HT_SINE(close)\n",
    "        df['TrendMode'] = ta.HT_TRENDMODE(close)\n",
    "        df['TrendLline'] = ta.HT_TRENDLINE(close)\n",
    "        \n",
    "        # Ta-Lib Price Transform\n",
    "        df['AvgPrice'] = ta.AVGPRICE(Open, high, low, close)\n",
    "        df['MedPrice'] = ta.MEDPRICE(high, low)\n",
    "        df['TypPrice'] = ta.TYPPRICE(high, low, close)\n",
    "        df['WClPrice'] = ta.WCLPRICE(high, low, close)\n",
    "        \n",
    "        # Ta-Lib Volatility indicators\n",
    "        df['ATR_14'] = ta.ATR(high, low, close)\n",
    "        df['NATR_14'] = ta.NATR(high, low, close)\n",
    "        df['TRange'] = ta.TRANGE(high, low, close)\n",
    "        \n",
    "        # Ta-Lib Pattern Recognition\n",
    "        df['2Crows'] = ta.CDL2CROWS(Open, high, low, close)\n",
    "        df['3BlCrows'] = ta.CDL3BLACKCROWS(Open, high, low, close)\n",
    "        df['3Inside'] = ta.CDL3INSIDE(Open, high, low, close)\n",
    "        df['3LStrike'] = ta.CDL3LINESTRIKE(Open, high, low, close)\n",
    "        df['3Outside'] = ta.CDL3OUTSIDE(Open, high, low, close)\n",
    "        df['3StInSouth'] = ta.CDL3STARSINSOUTH(Open, high, low, close)\n",
    "        df['3WhSoldiers'] = ta.CDL3WHITESOLDIERS(Open, high, low, close)\n",
    "        df['AbBaby'] = ta.CDLABANDONEDBABY(Open, high, low, close)\n",
    "        df['AdvBlock'] = ta.CDLADVANCEBLOCK(Open, high, low, close)\n",
    "        df['BHold'] = ta.CDLBELTHOLD(Open, high, low, close)\n",
    "        df['BrAway'] = ta.CDLBREAKAWAY(Open, high, low, close)\n",
    "        df['ClMarub'] = ta.CDLCLOSINGMARUBOZU(Open, high, low, close)\n",
    "        df['CBSwallow'] = ta.CDLCONCEALBABYSWALL(Open, high, low, close)\n",
    "        df['CAttack'] = ta.CDLCOUNTERATTACK(Open, high, low, close)\n",
    "        df['DCCover'] = ta.CDLDARKCLOUDCOVER(Open, high, low, close)\n",
    "        df['Doji'] = ta.CDLDOJI(Open, high, low, close)\n",
    "        df['DojiS'] = ta.CDLDOJISTAR(Open, high, low, close)\n",
    "        df['DFDoji'] = ta.CDLDRAGONFLYDOJI(Open, high, low, close)\n",
    "        df['EngPat'] = ta.CDLENGULFING(Open, high, low, close)\n",
    "        df['EDojiS'] = ta.CDLEVENINGDOJISTAR(Open, high, low, close)\n",
    "        df['EStar'] = ta.CDLEVENINGSTAR(Open, high, low, close)\n",
    "        df['GSSWhite'] = ta.CDLGAPSIDESIDEWHITE(Open, high, low, close)\n",
    "        df['GDoji'] = ta.CDLGRAVESTONEDOJI(Open, high, low, close)\n",
    "        df['Hammer'] = ta.CDLHAMMER(Open, high, low, close)\n",
    "        df['HMan'] = ta.CDLHANGINGMAN(Open, high, low, close)\n",
    "        df['Har'] = ta.CDLHARAMI(Open, high, low, close)\n",
    "        df['HarCr'] = ta.CDLHARAMICROSS(Open, high, low, close)\n",
    "        df['HWCdl'] = ta.CDLHIGHWAVE(Open, high, low, close)\n",
    "        df['Hik'] = ta.CDLHIKKAKE(Open, high, low, close)\n",
    "        df['HikMod'] = ta.CDLHIKKAKEMOD(Open, high, low, close)\n",
    "        df['HomPig'] = ta.CDLHOMINGPIGEON(Open, high, low, close)\n",
    "        df['I3Crows'] = ta.CDLIDENTICAL3CROWS(Open, high, low, close)\n",
    "        df['InNeck'] = ta.CDLINNECK(Open, high, low, close)\n",
    "        df['IHammer'] = ta.CDLINVERTEDHAMMER(Open, high, low, close)\n",
    "        df['Kicking'] = ta.CDLKICKING(Open, high, low, close)\n",
    "        df['KickingL'] = ta.CDLKICKINGBYLENGTH(Open, high, low, close)\n",
    "        df['LadBot'] = ta.CDLLADDERBOTTOM(Open, high, low, close)\n",
    "        df['LLDoji'] = ta.CDLLONGLEGGEDDOJI(Open, high, low, close)\n",
    "        df['LLCdl'] = ta.CDLLONGLINE(Open, high, low, close)\n",
    "        df['Maru'] = ta.CDLMARUBOZU(Open, high, low, close)\n",
    "        df['MatchLow'] = ta.CDLMATCHINGLOW(Open, high, low, close)\n",
    "        df['MatHold'] = ta.CDLMATHOLD(Open, high, low, close)\n",
    "        df['MDojiS'] = ta.CDLMORNINGDOJISTAR(Open, high, low, close)\n",
    "        df['MStar'] = ta.CDLMORNINGSTAR(Open, high, low, close)\n",
    "        df['OnNeck'] = ta.CDLONNECK(Open, high, low, close)\n",
    "        df['Piercing'] = ta.CDLPIERCING(Open, high, low, close)\n",
    "        df['RickshawM'] = ta.CDLRICKSHAWMAN(Open, high, low, close)\n",
    "        df['RF3Meth'] = ta.CDLRISEFALL3METHODS(Open, high, low, close)\n",
    "        df['SepLines'] = ta.CDLSEPARATINGLINES(Open, high, low, close)\n",
    "        df['ShStar'] = ta.CDLSHOOTINGSTAR(Open, high, low, close)\n",
    "        df['SLCdl'] = ta.CDLSHORTLINE(Open, high, low, close)\n",
    "        df['SpinTop'] = ta.CDLSPINNINGTOP(Open, high, low, close)\n",
    "        df['Stalled'] = ta.CDLSTALLEDPATTERN(Open, high, low, close)\n",
    "        df['StSandw'] = ta.CDLSTICKSANDWICH(Open, high, low, close)\n",
    "        df['Takuri'] = ta.CDLTAKURI(Open, high, low, close)\n",
    "        df['TasukiG'] = ta.CDLTASUKIGAP(Open, high, low, close)\n",
    "        df['Thrusting'] = ta.CDLTHRUSTING(Open, high, low, close)\n",
    "        df['Tristar'] = ta.CDLTRISTAR(Open, high, low, close)\n",
    "        df['Un3River'] = ta.CDLUNIQUE3RIVER(Open, high, low, close)\n",
    "        df['UG2Crows'] = ta.CDLUPSIDEGAP2CROWS(Open, high, low, close)\n",
    "        df['XG3Meth'] = ta.CDLXSIDEGAP3METHODS(Open, high, low, close)\n",
    "        \n",
    "        # Ta-Lib Statistic indicators\n",
    "        df['Beta_5'] = ta.BETA(high, low)\n",
    "        df['Corr_5'] = ta.CORREL(high, low)\n",
    "        df['LReg_14'] = ta.LINEARREG(close)\n",
    "        df['LRegAngle_14'] = ta.LINEARREG_ANGLE(close)\n",
    "        df['LRegInt_14'] = ta.LINEARREG_INTERCEPT(close)\n",
    "        df['LRegSlope_14'] = ta.LINEARREG_SLOPE(close)\n",
    "        df['StdDev__5'] = ta.STDDEV(close)\n",
    "        df['TSF_14'] = ta.TSF(close)\n",
    "        df['VAR_5'] = ta.VAR(close)\n",
    "        \n",
    "        # Ta-Lib Math Trasform indicators\n",
    "        df['ACOS'] = ta.ACOS(close)\n",
    "        df['ASIN'] = ta.ASIN(close)\n",
    "        df['ATAN'] = ta.ATAN(close)\n",
    "        df['CEIL'] = ta.CEIL(close)\n",
    "        df['COS'] = ta.COS(close)\n",
    "        df['COSH'] = ta.COSH(close)\n",
    "        df['EXP'] = ta.EXP(close)\n",
    "        df['FLOOR'] = ta.FLOOR(close)\n",
    "        df['LN'] = ta.LN(close)\n",
    "        df['Log10'] = ta.LOG10(close)\n",
    "        df['SIN'] = ta.SIN(close)\n",
    "        df['SINH'] = ta.SINH(close)\n",
    "        df['SQRT'] = ta.SQRT(close)\n",
    "        df['TAN'] = ta.TAN(close)\n",
    "        df['TANH'] = ta.TANH(close)\n",
    "        \n",
    "        # Ta-Lib Math indicators\n",
    "        df['ADD'] = ta.ADD(high, low)\n",
    "        df['DIV'] = ta.DIV(high, low)\n",
    "        df['MAX_30'] = ta.MAX(close)\n",
    "        df['MAXIndex_30'] = ta.MAXINDEX(close)\n",
    "        df['MIN_30'] = ta.MIN(close)\n",
    "        df['MINIndex_30'] = ta.MININDEX(close)\n",
    "        df['MULT'] = ta.MULT(high, low)\n",
    "        df['SUB'] = ta.SUB(high, low)\n",
    "        df['SUM_30'] = ta.SUM(close)\n",
    "        \n",
    "        # Add suffix '_TI' for if else above and avoid redundant computation\n",
    "        prices_path_TI = prices_path.replace('.csv', '_TI.csv')\n",
    "        df.to_csv(prices_path_TI)\n",
    "        os.remove(prices_path)\n",
    "        prices_path = prices_path_TI\n",
    "        print(\"Technical indicators computed. File saved as\", prices_path.split('/')[2] + '.')\n",
    "    \n",
    "    else:\n",
    "        print(\"\\nRelevant prices file with technical indicators already exists as\", prices_path.split('/')[2] + '.')\n",
    "        \n",
    "    return prices_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twitter posts download and manipulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next step is to handle the Twitter posts scraping. Here we will take advantage of multithreading and multiprocess computing programs. As the Python implementation (and so our classes created above for parallel processing) requires a pool of tasks and a worker that picks from that pool, this fits well with using parallel processing to scrape or manipulate simultaneously the different Twitter queries that the user passes in. To do so, then, we need to build a query for each twitter search.\n",
    "\n",
    "We will also split any \"scrape by most popular\" query in multiple queries as this has proven to be computationally expensive. We wil do so by setting a rule as to the number of Twitter posts requested depending on the timeframe and the number of max tweets passed by the user.\n",
    "\n",
    "Let's first manage the  creation of the folder for the raw scraped Twitter posts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=manage_data_directory)\n",
    "def manage_tweets_raw_folder(root):\n",
    "    \"\"\"\n",
    "    Scan \"data\" directory and create \"tweets_raw\" folder if not existing yet.\n",
    "    Return \"raw_tweets\" path.\n",
    "    \n",
    "    Arguments:\n",
    "        root: string, project's data root directory.\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_raw_folder = 'tweets_raw'\n",
    "    existing_folders = os.listdir(root)\n",
    "    tweets_raw_path = root + 'tweets_raw/'\n",
    "    if tweets_raw_folder not in existing_folders:\n",
    "        os.makedirs(tweets_raw_path)\n",
    "        print('\\n\"tweets_raw\" folder created in root \"data\" directory.')\n",
    "    else:\n",
    "        print('\\n\"tweets_raw\" folder already exists in root \"data\" directory.')\n",
    "    \n",
    "    return tweets_raw_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create a function that manages the \"scrape by most popular\" searches. After some attempts, i could infer that intervals below 2 years were handled fine in single thread procesing. However, longer intervals often failed downloading. This may be due to the Twitter API rejecting the request, or some status triggered within the snscrape API, which I did inspect fully.\n",
    "\n",
    "The condition used here is intended to replicate the experiment and keep a very conservative approach to ensure reliability in downloading the requested tweets, therefore an interval of 7 days is used, also considering that the download work in parallel processing (4 concurrent threads seemed to be a good compromise speed/reliability, so the Multithread_Executor has been set with a max of 4 threads). The only drawback with this setting is that a significant number of tweets chunks dataset will be produced. On the other hand, this is not a problem as the function \"concat_most_popular_tweets_chunks\" downstream will take care of aggregating in one unique dataset all the most popular tweets. Furthermore, the program is designed so that if the download fails halfway, a new run of the pipeline with the same download paramaters will take care of downloading solely what failed at the previous attempt. Then any failures on the most popular tweets download won't affect the next attempt and the task will be taken from the point it failed. If in doubt about an incomplete tweets dataset, just delete it manually from the folder and it will be downloaded at the next run.\n",
    "\n",
    "It is worth to note that the max number of tweets per day does not affect the search, as this number will just represent the final number of tweets selected at the end of the scraping after ranking them by the number of retweet that each post obtained. Please note that this behaviour has not been extensively tested and significantly different download parameters may result in further failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dates(since, until):\n",
    "    \"\"\"\n",
    "    Return list of tuples with start-end intervals resulting from splitting\n",
    "    the original period in multiple intervals. Size of intervals is determined\n",
    "    by the max_tweets_per_day number to avoid scraping failures.\n",
    "    \n",
    "    Arguments:\n",
    "        since: str, oldest content date\n",
    "        until: str, latest content date\n",
    "        max_tweets: int, max number of most popular tweets per day for each day into dates' interval passed\n",
    "    \"\"\"\n",
    "    \n",
    "    step = timedelta(7) # 7 days worth of tweets to increase chances of download completion without request failures\n",
    "    start_date = datetime.strptime(since, '%Y-%m-%d').date()\n",
    "    end_date = datetime.strptime(until, '%Y-%m-%d').date()\n",
    "    \n",
    "    dates = []\n",
    "    while (start_date <= end_date):\n",
    "        if start_date + step + timedelta(1) > end_date:\n",
    "            dates.append((str(start_date), str(end_date)))\n",
    "        else:\n",
    "            dates.append((str(start_date), str(start_date + step)))\n",
    "        start_date += step + timedelta(1)\n",
    "    \n",
    "    return dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "since = '2010-6-29'\n",
    "until = '2021-3-14'\n",
    "dates = split_dates(since, until)\n",
    "dates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the dates have been correctly split betweein the earliest and latest dates. We will use this function in the next function to build the Twitter posts filenames and queries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task we have to accomplish is scheduling the raw twitter posts scraping. We need to read into the download parameters provided and build the filenames for the Twitter posts and the related queries. We will then use these data to scan into the current content of the tweets_raw folder to decide what needs to be downloaded and what is already present, and remove any irrelevant files.\n",
    "\n",
    "We will divide this task in two functions. The first function will take as input the raw_tweets folder path and will return a list of pairs with the filenames and queries. The second function will scan into the \"tweets_raw\" folder to list what is still required to be downloaded and will pass to the download function which we'll build later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=manage_tweets_raw_folder)\n",
    "def session_details_builder(tweets_raw_path):\n",
    "    \"\"\"\n",
    "    Read in the download parameters for the twitter posts\n",
    "    Return a dict containing the \"tweets_raw\" folder path and\n",
    "    the filenames and queries pairs to download and save the data\n",
    "    \n",
    "    Argument:\n",
    "        raw_tweets_path: str, path to raw tweets folder\n",
    "    \"\"\"\n",
    "    \n",
    "    # Output data container\n",
    "    session_details = {'changes': True,\n",
    "                       'data_path' : 'data/',\n",
    "                       'prices_path' : 'data/prices/',\n",
    "                       'tweets_raw_path' : tweets_raw_path,\n",
    "                       'filenames_queries' : []}\n",
    "    \n",
    "    # Parameters for all\n",
    "    ticker = download_params['ticker'] + '_'\n",
    "    since = download_params['since']\n",
    "    if download_params['until'] == '':\n",
    "        until = datetime.strptime(str(datetime.now().date()), \"%Y-%m-%d\").date()\n",
    "    else:\n",
    "        until = download_params['until']\n",
    "        \n",
    "    # twitter_scrape_by_account filenames and queries\n",
    "    scrape_by_account = download_params['twitter_scrape_by_account']\n",
    "    if scrape_by_account:\n",
    "        for user in scrape_by_account:\n",
    "            filename = ticker + since.replace('-','') + '_' + until.replace('-','') + '_' + user\n",
    "            if scrape_by_account[user]['by_hashtag']:\n",
    "                filename += '_h_' + scrape_by_account[user]['search_keyword']\n",
    "            elif scrape_by_account[user]['search_keyword'] != '':\n",
    "                filename += '_t_' + scrape_by_account[user]['search_keyword']\n",
    "            filename += '_' + download_params['language'] + '.csv'\n",
    "            \n",
    "            query = ''\n",
    "            if scrape_by_account[user]['by_hashtag']:\n",
    "                query += '#' + scrape_by_account[user]['search_keyword'] + ' '\n",
    "            elif scrape_by_account[user]['search_keyword'] != '':\n",
    "                query += scrape_by_account[user]['search_keyword'] + ' '\n",
    "            query += 'from:' + user + ' '\n",
    "            query += 'since:' + since + ' '\n",
    "            query += 'until:' + until + ' '\n",
    "            query += 'lang:' + download_params['language']\n",
    "            \n",
    "            session_details['filenames_queries'].append((filename, query, ''))\n",
    "    \n",
    "    # twitter_scrape_by_most_popular filenames and queries\n",
    "    scrape_by_most_popular = download_params['twitter_scrape_by_most_popular']\n",
    "    if scrape_by_most_popular:\n",
    "        for search in scrape_by_most_popular:\n",
    "            max_tweets_per_day = scrape_by_most_popular[search]['max_tweets_per_day']\n",
    "            dates = split_dates(since, until)\n",
    "            n = 1\n",
    "            for interval in dates:\n",
    "                s = interval[0]\n",
    "                u = interval[1]\n",
    "                filename = ticker + s.replace('-','') + '_' + u.replace('-','')\n",
    "                if scrape_by_most_popular[search]['by_hashtag']:\n",
    "                    filename += '_h_' + scrape_by_most_popular[search]['search_keyword']\n",
    "                else:\n",
    "                    filename += '_t_' + scrape_by_most_popular[search]['search_keyword']\n",
    "                filename += '_' + download_params['language']\n",
    "                filename += f'_m{str(max_tweets_per_day)}tpd'\n",
    "                filename += f'_{str(n)}.csv'\n",
    "                n+=1\n",
    "                \n",
    "                query = ''\n",
    "                if scrape_by_most_popular[search]['by_hashtag']:\n",
    "                    query += '#' + scrape_by_most_popular[search]['search_keyword'] + ' '\n",
    "                else:\n",
    "                    query += scrape_by_most_popular[search]['search_keyword'] + ' '\n",
    "                query += 'since:' + s + ' '\n",
    "                query += 'until:' + u + ' '\n",
    "                query += 'lang:' + download_params['language']\n",
    "                \n",
    "                session_details['filenames_queries'].append((filename, query, max_tweets_per_day))\n",
    "                    \n",
    "    return session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function building the queries and filenames defined above and pushed into the pipeline, we are now ready to build the three functions which will be the parameters for the multi processing class we will use to download the tweets in a parallel computing fashion.\n",
    "<br>\n",
    "The three functions we need to build are:\n",
    "* make_tasks_pool --> takes the outstanding queries to download and returns an iterable of tasks (queries in this case)\n",
    "* worker --> picks from tasks pool query and downloads and saves tweets csv files\n",
    "* handle_outputs_pool --> returns dict of folder path and filenames downloaded\n",
    "\n",
    "<br>\n",
    "These functions will then be encapsulated in a \"download_tweets\" function by means of instantiating a Multithread_Executor object into the \"download_tweets\" functions, which will be pushed into the pipeline. This function will first scan into the folder and identify if there are oustanding queries to add to the tasks, and delete the non relevant ones. \n",
    "<br>\n",
    "<br>\n",
    "Let's go ahead writing these functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The make_tasks_pool function for tweets download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tasks_pool_download_tweets(to_do):\n",
    "    \"\"\"\n",
    "    Return a list of filename, query pairs representing\n",
    "    the outstanding one (to do), previously selected and\n",
    "    returned by the wrapping function\n",
    "    \n",
    "    Arguments:\n",
    "        to_do: dict, outstanding queries selected by the wrapping function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Slice the filenames_queries content from the to_do dict\n",
    "    folder_path = to_do['tweets_raw_path']\n",
    "    tasks_pool = [(folder_path, fq[0], fq[1], fq[2]) for fq in to_do['filenames_queries']]\n",
    "    return tasks_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The worker function for tweets download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_download_tweets(task):\n",
    "    \"\"\"\n",
    "    Download tweets related to the task query\n",
    "    Save as csv file\n",
    "    Return filename/query details\n",
    "    \n",
    "    Arguments:\n",
    "        task: tuple, task details (path to save to, filename, query, max tweets per day)\n",
    "              as returned by make_tasks_pool_tweets_download function\n",
    "    \"\"\"\n",
    "    \n",
    "    filename = task[1]\n",
    "    save_as = task[0] + filename\n",
    "    query = task[2]\n",
    "    max_tweets_per_day = task[3]\n",
    "    \n",
    "    print('Working on query: ', query)\n",
    "    \n",
    "    \n",
    "    # Handle \"all tweets\" search\n",
    "    if max_tweets_per_day == '':\n",
    "        \n",
    "        # Scrape Twitter with snscrape and append relevant data from tweets to our list\n",
    "        tweets = []\n",
    "        for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "            tweets.append([tweet.date.date(), tweet.content])\n",
    "        \n",
    "        # Make pandas dataframe to avoid issues with formatting and unescaped quotes, save as csv file\n",
    "        tweets_df = pd.DataFrame(tweets, columns=['Date', 'Content']).set_index(['Date'])\n",
    "        tweets_df.to_csv(save_as)\n",
    "    \n",
    "    \n",
    "    # Handle \"most popular tweets\" search\n",
    "    if isinstance(max_tweets_per_day, int):\n",
    "        tweets_df = pd.DataFrame()\n",
    "        since = re.search(r'(?<=since:)[0-9]+-[0-9]+-[0-9]+', query).group()\n",
    "        until = re.search(r'(?<=until:)[0-9]+-[0-9]+-[0-9]+', query).group()\n",
    "        start_date = datetime.strptime(since, \"%Y-%m-%d\").date()\n",
    "        end_date = datetime.strptime(until, \"%Y-%m-%d\").date()\n",
    "        while start_date <= end_date:\n",
    "            \n",
    "            # Scrape Twitter with snscrape and select most popular tweets, max number collected is max_tweets or whatever available if less\n",
    "            tweets = []\n",
    "            for i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n",
    "                tweets.append([tweet.date.date(), tweet.retweetCount, tweet.renderedContent])\n",
    "            to_append_tweets_df = pd.DataFrame(tweets, columns=['Date', 'Retweet_Count', 'Content'])\n",
    "            to_append_tweets_df = to_append_tweets_df.set_index(['Date', 'Retweet_Count']).sort_index(level=1, ascending=False)\n",
    "            to_append_tweets_df = to_append_tweets_df.droplevel(1)\n",
    "            idx_max = max_tweets_per_day\n",
    "            \n",
    "            # Limit to max_tweets or whatever number of tweets collected, if less than max_tweets\n",
    "            if to_append_tweets_df.shape[0] < max_tweets_per_day:\n",
    "                idx_max = to_append_tweets_df.shape[0]\n",
    "            tweets_df = tweets_df.append(to_append_tweets_df.iloc[:idx_max,:])\n",
    "            start_date += timedelta(days=1)\n",
    "            \n",
    "        # Save as csv file\n",
    "        tweets_df.to_csv(save_as)\n",
    "            \n",
    "    \n",
    "    print('Completed query: ', query)\n",
    "    # Return details of query completed (tuple of filename, query, max tweets per day)\n",
    "    return (filename, query, max_tweets_per_day)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The handle_outputs_pool function for tweets download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outputs_pool_download_tweets(outputs_pool):\n",
    "    \"\"\"\n",
    "    Return list of outputs from worker_tweets_download threads processing\n",
    "    Each element in the list contains a tuple of filename, query and\n",
    "    max tweets per day for each query executed\n",
    "    \n",
    "    Arguments:\n",
    "        outputs_pool: list, as returned by worker_tweets_download threads\n",
    "    \"\"\"\n",
    "    \n",
    "    return outputs_pool"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The download_tweets wrapper function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thi is the function that will be pushed into the pipeline and will depend on the \"raw_tweets_filenams_and_queries\" function. As such, it has to pick the output from that function, which is a dict containing details of the folder path, the filenames with wich to save the tweets files, and the queries to download the tweets. As mentioned above, this function will scan the folder to identify the queries that need to be downloaded and will instiantiate and run a \"Multithread_Executor\" object made of the functions we just defined, if there are any queires to be downloaded, based on the current \"download_params\" dict  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=session_details_builder)\n",
    "def download_tweets(session_details):\n",
    "    \"\"\"\n",
    "    Scan \"tweets_raw\" folder, identify tasks to do and download\n",
    "    tweets if there are new queries to download.\n",
    "    Return updated session_details_builder as per current session\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"session_details_builder\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Check for relevant files already existing and build to_do list\n",
    "    print('\\nScanning the \"tweets_raw\" folder and downloading new tweets if required...')\n",
    "    tweets_raw_folder_path = session_details['tweets_raw_path']\n",
    "    post_concat_session_details = copy.deepcopy(session_details)\n",
    "    existing_files = [file.split('/')[2] for file in glob.glob(tweets_raw_folder_path + '*')]\n",
    "    to_do = copy.deepcopy(session_details)\n",
    "    outputs = to_do['filenames_queries']\n",
    "    \n",
    "    # If current requested files already in existing_files, update to_do\n",
    "    if existing_files:\n",
    "        to_do['filenames_queries'] = []\n",
    "        #concat_most_popular_filenames = set()\n",
    "        for task in session_details['filenames_queries']:\n",
    "            if task[2] == '':\n",
    "                if task[0] not in existing_files:\n",
    "                    to_do['filenames_queries'].append(task)\n",
    "            elif isinstance(task[2], int):\n",
    "                if task[0] in existing_files:\n",
    "                    pass\n",
    "                # Check for potential \"most popular\" search type files already concatenated in the next step\n",
    "                else:\n",
    "                    i = task[0].find('tpd')\n",
    "                    replace_with_date_since = download_params['since'].replace('-', '')\n",
    "                    replace_with_date_until = download_params['until'].replace('-', '')\n",
    "                    date_to_replace_since = re.findall(r'([0-9]{8,8})', task[0])[0]\n",
    "                    date_to_replace_until = re.findall(r'([0-9]{8,8})', task[0])[1]\n",
    "                    concat_most_popular_filename = task[0][:i+3] + '.csv'\n",
    "                    concat_most_popular_filename = concat_most_popular_filename.replace(date_to_replace_since, replace_with_date_since)\n",
    "                    concat_most_popular_filename = concat_most_popular_filename.replace(date_to_replace_until, replace_with_date_until)\n",
    "                    if concat_most_popular_filename in existing_files:\n",
    "                        new_query = task[1]\n",
    "                        max_tweets_per_day = task[2]\n",
    "                        date_to_replace_since = re.search(r'(?<=since:)[0-9]+-[0-9]+-[0-9]+', new_query).group()\n",
    "                        date_to_replace_until = re.search(r'(?<=until:)[0-9]+-[0-9]+-[0-9]+', new_query).group()\n",
    "                        new_query = new_query.replace(date_to_replace_since, download_params['since'])\n",
    "                        new_query = new_query.replace(date_to_replace_until, download_params['until'])\n",
    "                        new_task = (concat_most_popular_filename, new_query, max_tweets_per_day)\n",
    "                        if new_task not in post_concat_session_details['filenames_queries']:\n",
    "                            post_concat_session_details['filenames_queries'].append(new_task)\n",
    "                        if task in post_concat_session_details['filenames_queries']:\n",
    "                            post_concat_session_details['filenames_queries'].remove(task)\n",
    "                        #concat_most_popular_filenames.add(concat_most_popular_filename)\n",
    "                    elif concat_most_popular_filename not in existing_files:\n",
    "                        to_do['filenames_queries'].append(task)\n",
    "    \n",
    "    # If there is at least one task in to_do, instantiate Multithread_Executor obj and execute download\n",
    "    if to_do['filenames_queries']:\n",
    "        multithread_executor = Multithread_Executor(to_do,\n",
    "                                                    make_tasks_pool_download_tweets,\n",
    "                                                    worker_download_tweets,\n",
    "                                                    handle_outputs_pool_download_tweets)\n",
    "        outputs = multithread_executor.execute()\n",
    "    \n",
    "    # If there were any existing files, check for non relevant\n",
    "    if existing_files:\n",
    "        to_delete_files = []\n",
    "        new_files = [task[0] for task in post_concat_session_details['filenames_queries']]\n",
    "        #new_files.extend(concat_most_popular_filenames)\n",
    "        for file in existing_files:\n",
    "            if file not in new_files:\n",
    "                to_delete_files.append(file)\n",
    "\n",
    "        # if irrelevant files in the directory, delete them\n",
    "        if to_delete_files:\n",
    "            for file in to_delete_files:\n",
    "                file = tweets_raw_folder_path + file\n",
    "                os.remove(file)\n",
    "            print('Irrelevant existing raw tweets files deleted.')\n",
    "    \n",
    "    # Return updated session_details with tasks actually downloaded, print any files due but not downloaded\n",
    "    new_session_details = copy.deepcopy(post_concat_session_details)\n",
    "    post_download_and_deletion_files = [file.split('/')[2] for file in glob.glob(tweets_raw_folder_path + '*')]\n",
    "    post_download_and_deletion_tasks = []\n",
    "    non_downloaded_tasks = []\n",
    "    for task in post_concat_session_details['filenames_queries']:\n",
    "        if task[0] in post_download_and_deletion_files:\n",
    "            post_download_and_deletion_tasks.append(task)\n",
    "        else:\n",
    "            non_downloaded_tasks.append(task)\n",
    "    new_session_details['filenames_queries'] = post_download_and_deletion_tasks\n",
    "    if non_downloaded_tasks:\n",
    "        l = len(non_downloaded_tasks)\n",
    "        print(f\"\\nFailed download of quer{'y' if l==1 else 'ies'}:\")\n",
    "        for task in non_downloaded_tasks:\n",
    "            print(task[1])        \n",
    "        print('\\n')\n",
    "    \n",
    "    if to_do['filenames_queries']:\n",
    "        print('Scan of \"tweets_raw\" folder and tweets files download complete.')\n",
    "    if not to_do['filenames_queries']:\n",
    "        print('Scan of \"tweets_raw\" folder complete.')\n",
    "    if (not to_do['filenames_queries']) and (not to_delete_files):\n",
    "        print('All relevant raw tweets files already existing. No Download executed.')\n",
    "        new_session_details['changes'] = False\n",
    "        \n",
    "    return new_session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate the tweets dataset chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have finalised the download process functions, we need to concatenate the tweets dataset downloaded with a \"most popular\" search. We in fact designed the \"raw_tweets_filenams_and_queries\" function so that it would split these tweets query in several chunks if some conditions were verified, such as the length of the time period and the number of tweets per day. Therefore, if the download ends with chunks for these query (which is likely due to long time periods usually used to train and test the ML model), we then need to concatenate the chunks in one single dataset.\n",
    "\n",
    "Let's design a function to accomplish the concatenation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=download_tweets)\n",
    "def concat_most_popular_tweets_chunks(session_details):\n",
    "    \"\"\"\n",
    "    Concatenate the tweets downloaded as chunks for the \"most popular\" query type.\n",
    "    Flag tweets files not found due to failed download.\n",
    "    Return the session_details updated post-concatenation.\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"download_tweets\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    if session_details['changes']:\n",
    "        tweets_raw_folder_path = session_details['tweets_raw_path']\n",
    "        new_session_details = copy.deepcopy(session_details)\n",
    "        to_concatenate = dict()\n",
    "        files_to_remove = []\n",
    "        replace_with_date_since = download_params['since'].replace('-', '')\n",
    "        replace_with_date_until = download_params['until'].replace('-', '')\n",
    "    \n",
    "        # Identify \"most popular\" type queries to concatenate\n",
    "        for task in session_details['filenames_queries']:\n",
    "            if isinstance(task[2], int):\n",
    "                i = task[0].find('tpd')        \n",
    "                if task[0][-5] == '1':\n",
    "                    concat_filename = task[0][:i+3] + '.csv'\n",
    "                    date_to_replace_until = re.findall(r'([0-9]{8,8})', task[0])[1]\n",
    "                    concat_filename = concat_filename.replace(date_to_replace_until, replace_with_date_until)\n",
    "                    to_concatenate[concat_filename] = [task]\n",
    "                    new_session_details['filenames_queries'].remove(task)\n",
    "                elif (task[0][-5] != '1') and (task[0][-5] != 'd'):\n",
    "                    concat_filename = task[0][:i+3] + '.csv'\n",
    "                    date_to_replace_since = re.findall(r'([0-9]{8,8})', task[0])[0]\n",
    "                    date_to_replace_until = re.findall(r'([0-9]{8,8})', task[0])[1]\n",
    "                    concat_filename = concat_filename.replace(date_to_replace_since, replace_with_date_since)\n",
    "                    concat_filename = concat_filename.replace(date_to_replace_until, replace_with_date_until)\n",
    "                    to_concatenate[concat_filename].append(task)\n",
    "                    new_session_details['filenames_queries'].remove(task)\n",
    "    \n",
    "        # If identified chunks of same query files to concatenate, concatenate them\n",
    "        if to_concatenate:\n",
    "            for concat_filename in to_concatenate:\n",
    "                if len(to_concatenate[concat_filename]) == 1:\n",
    "                    task = to_concatenate[concat_filename][0]\n",
    "                    new_task = (concat_filename, task[1], task[2])\n",
    "                    new_session_details['filenames_queries'].append(new_task)\n",
    "                    old_filename = to_concatenate[concat_filename][0][0]\n",
    "                    old_filename_path = tweets_raw_folder_path + old_filename        \n",
    "                    new_filename_path = tweets_raw_folder_path + concat_filename\n",
    "                    os.rename(old_filename_path, new_filename_path)\n",
    "    \n",
    "                elif len(to_concatenate[concat_filename]) > 1:\n",
    "                    concatenated_tweets = pd.DataFrame()\n",
    "                    for task in to_concatenate[concat_filename]:\n",
    "                        current_filename = tweets_raw_folder_path + task[0]\n",
    "                        to_append = pd.read_csv(current_filename, engine='python', index_col=[0])\n",
    "                        concatenated_tweets = concatenated_tweets.append(to_append)\n",
    "                        files_to_remove.append(task[0])\n",
    "    \n",
    "                    new_query = to_concatenate[concat_filename][0][1]\n",
    "                    max_tweets_per_day = to_concatenate[concat_filename][0][2]\n",
    "                    date_to_replace_until = re.search(r'(?<=until:)[0-9]+-[0-9]+-[0-9]+', new_query).group()\n",
    "                    new_query = new_query.replace(date_to_replace_until, download_params['until'])\n",
    "                    new_task = (concat_filename, new_query, max_tweets_per_day)\n",
    "                    new_session_details['filenames_queries'].append(new_task)\n",
    "                    concat_filename_path = tweets_raw_folder_path + concat_filename\n",
    "                    concatenated_tweets.to_csv(concat_filename_path)\n",
    "    \n",
    "        # Remove initial chunks files already concatenated and saved as new file\n",
    "        if files_to_remove:\n",
    "            for file in files_to_remove:\n",
    "                to_remove = tweets_raw_folder_path + file\n",
    "                os.remove(to_remove)\n",
    "        \n",
    "        return new_session_details\n",
    "    \n",
    "    elif not session_details['changes']:\n",
    "        return session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unstack all tweets files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now all the queries downloaded (and concatenated in a single file in case of \"most popular\" query type initially downloaded as chunks).\n",
    "We can now unstack the rows in each file, i.e. concatenate all the tweets strings aggregating on the datetime index.\n",
    "We will use multiprocessing for this task, then we'll need to write the functions to fit the Multiprocess_Executor object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tasks_pool_unstack_tweets(to_do):\n",
    "    \"\"\"\n",
    "    Scan \"tweets raw\" and \"tweets_unstacked\" folders and return list of\n",
    "    tuples with filepaths to pick in \"tweets_raw\" and save in \"tweets_unstacked\"\n",
    "    \n",
    "    Arguments:\n",
    "        to_do: dict, as provided by \"unstack_tweets\" wrapper function\n",
    "    \"\"\"\n",
    "    \n",
    "    to_do_updated = []\n",
    "    tweets_raw_path = to_do['tweets_raw_path']\n",
    "    tweets_unstacked_path = to_do['tweets_unstacked_path']\n",
    "    tweets_raw_existing_files = [file.split('/')[2] for file in glob.glob(tweets_raw_path + '*')]\n",
    "    tweets_unstacked_existing_files = [file.split('/')[2] for file in glob.glob(tweets_unstacked_path + '*')]\n",
    "    \n",
    "    # Pick only files that haven't yet been unstacked and saved into the \"tweets_unstacked\" folder\n",
    "    for tweets_raw_filename in to_do['tweets_raw_files']:\n",
    "        tweets_unstacked_filename = tweets_raw_filename.replace('.csv', '_unstacked.csv')\n",
    "        if tweets_unstacked_filename not in tweets_unstacked_existing_files:\n",
    "            tweets_raw_file_path = tweets_raw_path + '/' + tweets_raw_filename\n",
    "            tweets_unstacked_file_path = tweets_unstacked_path + '/' + tweets_unstacked_filename\n",
    "            to_do_updated.append((tweets_raw_file_path, tweets_unstacked_file_path))\n",
    "        \n",
    "    return to_do_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_unstack_tweets(task):\n",
    "    \"\"\"\n",
    "    Unstack tweets data by concatenating strings aggregating the datetime index.\n",
    "    Save to csv file.\n",
    "    \n",
    "    Arguments:\n",
    "        task: tuple, tweets_raw_file_path and tweets_unstacked_file_path from\n",
    "              make_tasks_pool_unstack_tweets function\n",
    "    \"\"\"\n",
    "    \n",
    "    tweets_raw_file_path = task[0]\n",
    "    tweets_unstacked_file_path = task[1]\n",
    "    df = pd.read_csv(tweets_raw_file_path, engine='python', index_col=[0])\n",
    "    \n",
    "    # Remove nan to avoid issues with string manipulation below\n",
    "    df = df.dropna()\n",
    "    \n",
    "    # Remove urls which may cause issues identifying duplicates below\n",
    "    df['Content'] = df['Content'].apply(lambda x: re.sub(r'https?:\\/\\/[A-z.\\/0-9-]*', \"\", x))\n",
    "    \n",
    "    df = df.drop_duplicates()\n",
    "    \n",
    "    # Concatenate same index tweets in one string and save as csv file\n",
    "    df = df.groupby(by=df.index).sum()\n",
    "    df.to_csv(tweets_unstacked_file_path)    \n",
    "    \n",
    "    tweets_unstacked_filename = tweets_unstacked_file_path.split('/')[2]\n",
    "    return tweets_unstacked_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outputs_pool_unstack_tweets(outputs_pool):\n",
    "    \"\"\"\n",
    "    Just return the outputs_pool (list of filenames of each\n",
    "    tweets_unstacked_filename saved.\n",
    "    \n",
    "    Arguments:\n",
    "        outputs_pool: list, tweets_unstacked_filename for unstacked tweets files\n",
    "    \"\"\"\n",
    "    \n",
    "    return outputs_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=concat_most_popular_tweets_chunks)\n",
    "def unstack_tweets(session_details):\n",
    "    \"\"\"\n",
    "    Create \"tweets_unstacked\" folder if not existing.\n",
    "    Scan \"tweets_raw\" amnd \"tweets_unstacked\" folder, identify files to unstack\n",
    "    and execute unstacking if needed.\n",
    "    Return updated session_details dict as per current session\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"concat_most_popular_tweets_chunks\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the \"tweets_unstacked\" folder if it doesn't exists\n",
    "    tweets_raw_folder = session_details['tweets_raw_path']\n",
    "    existing_folders = os.listdir('data')\n",
    "    tweets_unstacked_folder = 'tweets_unstacked'\n",
    "    data_path = session_details['data_path']\n",
    "    session_details['tweets_unstacked_path'] = data_path + tweets_unstacked_folder + '/'\n",
    "    tweets_unstacked_path = session_details['tweets_unstacked_path']\n",
    "    if tweets_unstacked_folder not in existing_folders:\n",
    "        os.makedirs(tweets_unstacked_path)\n",
    "        print('\\n\"tweets_unstacked\" folder created in root \"data\" directory.')\n",
    "    else:\n",
    "        print('\\n\"tweets_unstacked\" folder already exists in root \"data\" directory.')\n",
    "    \n",
    "    # Add the existing files in the \"tweets_raw\" to the current session details file\n",
    "    new_session_details = copy.deepcopy(session_details)\n",
    "    tweets_raw_path = new_session_details['tweets_raw_path']\n",
    "    new_session_details['tweets_raw_files'] = [file.split('/')[2] for file in glob.glob(tweets_raw_path + '*')]\n",
    "    \n",
    "    # Check if any files in \"raw_tweets\" folder need to be unstacked\n",
    "    print('Scanning the \"tweets_raw\" and \"tweets_unstacked\" folders to find tweets files to be unstacked...')\n",
    "    tweets_unstacked_existing_files = [file.split('/')[2] for file in glob.glob(tweets_unstacked_path + '*')]\n",
    "    for tweets_raw_filename in new_session_details['tweets_raw_files']:\n",
    "        tweets_unstacked_filename = tweets_raw_filename.replace('.csv', '_unstacked.csv')\n",
    "        if tweets_unstacked_filename not in tweets_unstacked_existing_files:\n",
    "            new_session_details['changes'] = True\n",
    "            break\n",
    "    \n",
    "    # If there are files to unstack, instantiate Multiprocess_Executor object and execute unstacking\n",
    "    outputs = []\n",
    "    if new_session_details['changes']:\n",
    "        to_do = copy.deepcopy(new_session_details)\n",
    "        multiprocess_executor = Multiprocess_Executor(to_do,\n",
    "                                                    make_tasks_pool_unstack_tweets,\n",
    "                                                    worker_unstack_tweets,\n",
    "                                                    handle_outputs_pool_unstack_tweets)\n",
    "        outputs = multiprocess_executor.execute()\n",
    "    \n",
    "    # Check for non-relevant unstacked files to remove \n",
    "    files_to_remove = []\n",
    "    for file in [file.split('/')[2] for file in glob.glob(tweets_unstacked_path + '*')]:\n",
    "        if file.replace('_unstacked.csv', '.csv') not in new_session_details['tweets_raw_files']:\n",
    "            files_to_remove.append(file)\n",
    "    if files_to_remove:\n",
    "        for file in files_to_remove:\n",
    "            os.remove(tweets_unstacked_path + file)\n",
    "        new_session_details['changes'] = True\n",
    "        print('Irrelevant existing unstacked tweets files deleted.')\n",
    "          \n",
    "    # Update session details file adding the files in the \"tweets_unstacked\" folder\n",
    "    new_session_details['tweets_unstacked_files'] = [file.split('/')[2] for file in glob.glob(tweets_unstacked_path + '*')]\n",
    "    \n",
    "    if outputs:\n",
    "        print('Scan of \"tweets_raw\" and \"tweets_unstacked\" folders complete and unstacking executed.')\n",
    "    if not outputs:\n",
    "        print('Scan of \"tweets_raw\" and \"tweets_unstacked\" folders complete.')\n",
    "    if (not outputs) and (not files_to_remove):\n",
    "          print('All relevant unstacked tweets files already existing. No unstacking executed.')\n",
    "    \n",
    "    return new_session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Merge all tweets datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tweets datasets unstacked, we can now proceed to merge them all together in a unique dataset. This steps consists onnce again in concatenating in one string the content of each dataset, aggregating on the same datetime index, thus being technically an outer join.\n",
    "We don't need need parallel processing here as the operation should not be computationally expensive. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=unstack_tweets)\n",
    "def merge_all_unstacked_tweets(session_details):\n",
    "    \"\"\"\n",
    "    Check for changes upstream in the tweets queries and merge unstacked\n",
    "    tweets datasets if required.\n",
    "    Return updated session_details dict as per current session.\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"unstack_tweets\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the \"tweets_merged\" folder if it doesn't exists\n",
    "    tweets_unstacked_path = session_details['tweets_unstacked_path']\n",
    "    data_path = session_details['data_path']\n",
    "    existing_folders = os.listdir(data_path)\n",
    "    tweets_merged_folder = 'tweets_merged'\n",
    "    tweets_merged_path = data_path + tweets_merged_folder + '/'\n",
    "    session_details['tweets_merged_path'] = tweets_merged_path\n",
    "    new_session_details = copy.deepcopy(session_details)\n",
    "    if tweets_merged_folder not in existing_folders:\n",
    "        os.makedirs(tweets_merged_path)\n",
    "        print('\\n\"tweets_merged\" folder created in root \"data\" directory.')\n",
    "    else:\n",
    "        print('\\n\"tweets_merged\" folder already exists in root \"data\" directory.')\n",
    "    \n",
    "    # Check if merging is required, delete existing tweets merged file if upstream changes require new merged file\n",
    "    print('Checking if there have been any changes in the tweets queries. Unstacked tweets will be merged again if required...')\n",
    "    existing_tweets_merged_file = [file.split('/')[2] for file in glob.glob(session_details['tweets_merged_path'] + '*')]\n",
    "    if (not existing_tweets_merged_file) or session_details['changes']:\n",
    "        if existing_tweets_merged_file:\n",
    "            os.remove(tweets_merged_path + existing_tweets_merged_file[0])\n",
    "            print('Deleted existing merged tweets dataset.')\n",
    "        # Build new tweets merged filename\n",
    "        ticker = download_params['ticker'] + '_'\n",
    "        since = download_params['since'].replace('-', '') + '_'\n",
    "        until = download_params['until'].replace('-', '') + '_'\n",
    "        new_tweets_merged_filename = ticker + since + until + 'merged.csv'\n",
    "        new_tweets_merged_path = session_details['tweets_merged_path'] + new_tweets_merged_filename\n",
    "        existing_tweets_unstacked_files = session_details['tweets_unstacked_files']\n",
    "        initial_to_join_file_path = tweets_unstacked_path + existing_tweets_unstacked_files[0]\n",
    "        df = pd.read_csv(initial_to_join_file_path, engine='python', index_col=[0])\n",
    "        for file in existing_tweets_unstacked_files[1:]:\n",
    "            to_join_file_path = tweets_unstacked_path + file\n",
    "            df_to_join = pd.read_csv(to_join_file_path, engine='python', index_col=[0])\n",
    "            df = df.join(df_to_join, how='outer', rsuffix='_1')\n",
    "            df = df.replace(np.nan, '', regex=True)\n",
    "            df['Content'] = df['Content'] + df['Content_1']\n",
    "            df = df.iloc[:,0].to_frame()\n",
    "        df = df.replace('\\x00','')\n",
    "        df.to_csv(new_tweets_merged_path)\n",
    "        print(f'Completed merging unstacked tweets datasets.\\nFile saved in \"tweets_merged\" folder as {new_tweets_merged_filename}.')\n",
    "    \n",
    "    elif existing_tweets_merged_file and (not session_details['changes']):\n",
    "        print('Relevant merged tweets file already exists in \"tweets_merged\" folder. No neeed to merge again.')\n",
    "    \n",
    "    new_session_details['tweets_merged_file'] = [file.split('/')[2] for file in glob.glob(tweets_merged_path + '*')][0]\n",
    "    \n",
    "    return new_session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute the sentiment scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the tweets datasets merged in one unique file we can now proceed to compute the sentiment scores on the tweets. As this task may be quite expensive in terms of computation, we are going to take advantage once again of parallel processsing and we will design the usual functions to fit into the a Multiprocess_Executor object, which in turn will be instantiated in a wrapper function for this specific task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_tasks_pool_tweets_sentiment_scores(to_do):\n",
    "    \"\"\"\n",
    "    Return the list of tasks for the tweets sentiment score worker function as\n",
    "    tuples of the new file path (used later to save once merged again) and one\n",
    "    chunk of the tweets merged dataframe\n",
    "    \n",
    "    Arguments:\n",
    "        to_do: dict, as provided by \"compute_tweets_sentiment_scores\" wrapper function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Split the tweets merged dataframe in as many parts as cores on the processor\n",
    "    n_splits = mp.cpu_count()\n",
    "    tasks_pool = []\n",
    "    tweets_sentiment_scores_folder_path = to_do['tweets_sentiment_scores_path']\n",
    "    tweets_sentiment_score_filename = to_do['tweets_sentiment_scores_file']\n",
    "    tweets_sentiment_scores_file_path = tweets_sentiment_scores_folder_path + tweets_sentiment_score_filename\n",
    "    tweets_merged_folder_path = to_do['tweets_merged_path']\n",
    "    tweets_merged_filename = to_do['tweets_merged_file']\n",
    "    tweets_merged_file_path = tweets_merged_folder_path + tweets_merged_filename\n",
    "    tweets_merged_df = pd.read_csv(tweets_merged_file_path, index_col=[0])\n",
    "    tweets_merged_df_splits = np.array_split(tweets_merged_df, n_splits)\n",
    "    \n",
    "    # Build tasks as tuples of tweets sentiment scores file path and one chunk of the split dataframe\n",
    "    for df in tweets_merged_df_splits:\n",
    "        if not df.empty:\n",
    "            task = (tweets_sentiment_scores_file_path, df)\n",
    "            tasks_pool.append(task)\n",
    "    \n",
    "    return tasks_pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def worker_tweets_sentiment_scores(task):\n",
    "    \"\"\"\n",
    "    Compute the sentiment scores on passed dataframe.\n",
    "    Add 4 columns for negative, neutral, positive and compund scores\n",
    "    drop the original tweets string column.\n",
    "    Return the dataframe with sentiments scores columns only\n",
    "    \n",
    "    Arguments:\n",
    "        task: tuple, list of tuples, full filepath to save merged datframe of tweets sentiment scores\n",
    "              and one chunk of the tweets merged datraframe\n",
    "    \"\"\"\n",
    "    \n",
    "    df = task[1]\n",
    "    \n",
    "    # Instantiate sentiment scores analyser object, compute the scores\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    df['Tweets_Neg_Score'] = [analyzer.polarity_scores(content)['neg'] for content in df['Content']]\n",
    "    df['Tweets_Neu_Score'] = [analyzer.polarity_scores(content)['neu'] for content in df['Content']]\n",
    "    df['Tweets_Pos_Score'] = [analyzer.polarity_scores(content)['pos'] for content in df['Content']]\n",
    "    df['Tweets_Com_Score'] = [analyzer.polarity_scores(content)['compound'] for content in df['Content']]\n",
    "    df = df.drop(['Content'], axis=1)\n",
    "    \n",
    "    completed_task = (task[0], df)\n",
    "    \n",
    "    return completed_task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_outputs_pool_tweets_sentiment_scores(outputs_pool):\n",
    "    \"\"\"\n",
    "    Append all dataframe chunks with computed sentiment scores\n",
    "    and save in the \"tweets_sentiment_scores\" folder\n",
    "    \n",
    "    Arguments:\n",
    "        outputs_pool: list of tuples, full filepath to save merged datframe of tweets sentiment scores\n",
    "        and one chunk of the tweets merged datraframe  \n",
    "    \"\"\"\n",
    "    \n",
    "    # Merge all dataframe chunks with tweets sentiment scores\n",
    "    tweets_sentiment_scores_file_path = outputs_pool[0][0]\n",
    "    df = outputs_pool[0][1]\n",
    "    for output in outputs_pool[1:]:\n",
    "        to_append = output[1]\n",
    "        df = df.append(to_append)\n",
    "        \n",
    "    df = df.sort_index()\n",
    "    df.to_csv(tweets_sentiment_scores_file_path)\n",
    "    \n",
    "    return tweets_sentiment_scores_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=merge_all_unstacked_tweets)\n",
    "def compute_tweets_sentiment_scores(session_details):\n",
    "    \"\"\"\n",
    "    Check for changes upstream in the tweets queries and merge unstacked\n",
    "    tweets datasets if required.\n",
    "    Return updated session_details dict as per current session.\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"unstack_tweets\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the \"tweets_sentiment_scores\" folder if it doesn't exists\n",
    "    tweets_merged_path = session_details['tweets_merged_path']\n",
    "    data_path = session_details['data_path']\n",
    "    existing_folders = os.listdir(data_path)\n",
    "    tweets_sentiment_scores_folder = 'tweets_sentiment_scores'\n",
    "    tweets_sentiment_scores_path = data_path + tweets_sentiment_scores_folder + '/'\n",
    "    session_details['tweets_sentiment_scores_path'] = tweets_sentiment_scores_path\n",
    "    new_session_details = copy.deepcopy(session_details)\n",
    "    if tweets_sentiment_scores_folder not in existing_folders:\n",
    "        os.makedirs(tweets_sentiment_scores_path)\n",
    "        print('\\n\"tweets_sentiment_scores\" folder created in root \"data\" directory.')\n",
    "    else:\n",
    "        print('\\n\"tweets_sentiment_scores\" folder already exists in root \"data\" directory.')\n",
    "    \n",
    "    # Check if sentiment scores computing is required, delete existing tweets sentiment scores file if upstream changes require new computation\n",
    "    print('Checking if there have been any changes in the tweets queries. Sentiment scores will be computed again if required...')\n",
    "    existing_tweets_sentiment_scores_file = [file.split('/')[2] for file in glob.glob(session_details['tweets_sentiment_scores_path'] + '*')]\n",
    "    if (not existing_tweets_sentiment_scores_file) or session_details['changes']:\n",
    "        if existing_tweets_sentiment_scores_file:\n",
    "            os.remove(tweets_sentiment_scores_path + existing_tweets_sentiment_scores_file[0])\n",
    "            print('Deleted existing tweets sentiment scores dataset.')\n",
    "        # Build new tweets merged filename\n",
    "        ticker = download_params['ticker'] + '_'\n",
    "        since = download_params['since'].replace('-', '') + '_'\n",
    "        until = download_params['until'].replace('-', '') + '_'\n",
    "        new_tweets_sentiment_scores_filename = ticker + since + until + 'merged_ss.csv'\n",
    "        new_tweets_sentiment_scores_path = session_details['tweets_merged_path'] + new_tweets_sentiment_scores_filename\n",
    "        new_session_details['tweets_sentiment_scores_file'] = new_tweets_sentiment_scores_filename\n",
    "        to_do = copy.deepcopy(new_session_details)\n",
    "        multiprocess_executor = Multiprocess_Executor(to_do,\n",
    "                                                    make_tasks_pool_tweets_sentiment_scores,\n",
    "                                                    worker_tweets_sentiment_scores,\n",
    "                                                    handle_outputs_pool_tweets_sentiment_scores)\n",
    "        outputs = multiprocess_executor.execute()\n",
    "        \n",
    "        print('Completed computation of tweets sentiment scores.')\n",
    "        print(f'File saved in \"tweets_sentiment_scores\" folder as {new_tweets_sentiment_scores_filename}.')\n",
    "    \n",
    "    elif existing_tweets_sentiment_scores_file and (not session_details['changes']):\n",
    "        print('Relevant tweets sentiment scores file already exists in \"tweets_sentiment_scores\" folder. No neeed to compute sentimentt scores again.')\n",
    "    \n",
    "    new_session_details['tweets_sentiment_scores_file'] = [file.split('/')[2] for file in glob.glob(tweets_sentiment_scores_path + '*')][0]\n",
    "    \n",
    "    return new_session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate prices with tweets sentiment scores in final dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point we just need to concatenate the prices dataset and technical indicators with the tweets sentiment scores dataset in our final dataset which will be used to compute the trading strategy signals and train the machine learning algorithm.\n",
    "We do not need parallel processing for this ask as it is a simple lef join on the datetime index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@TSLA_data_pipeline.task(depends_on=compute_tweets_sentiment_scores)\n",
    "def join_prices_TI_and_tweets_sentiment_scores(session_details):\n",
    "    \"\"\"\n",
    "    Join the prices dataset with technical indicators together with the tweets\n",
    "    sentiment scores and save to csv file.\n",
    "    \n",
    "    Arguments:\n",
    "        session_details: dict, as returned by \"compute_tweets_sentiment_scores\" function\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create the \"prices_TI_sentiment_scores\" folder if it doesn't exists\n",
    "    data_path = session_details['data_path']\n",
    "    existing_folders = os.listdir(data_path)\n",
    "    prices_TI_path = session_details['prices_path']\n",
    "    session_details['prices_TI_file'] = glob.glob(prices_TI_path + '*')[0].replace(prices_TI_path, '')\n",
    "    prices_TI_sentiment_scores_folder = 'prices_TI_sentiment_scores'\n",
    "    prices_TI_sentiment_scores_path = data_path + prices_TI_sentiment_scores_folder + '/'\n",
    "    session_details['prices_TI_sentiment_scores_path'] = prices_TI_sentiment_scores_path\n",
    "    new_session_details = copy.deepcopy(session_details)\n",
    "    if prices_TI_sentiment_scores_folder not in existing_folders:\n",
    "        os.makedirs(prices_TI_sentiment_scores_path)\n",
    "        print('\\n\"prices_TI_sentiment_scores\" folder created in root \"data\" directory.')\n",
    "    else:\n",
    "        print('\\n\"prices_TI_sentiment_scores\" folder already exists in root \"data\" directory.')\n",
    "    \n",
    "    # Check if new final dataset id required, delete existing file if upstream changes require to join all data again\n",
    "    print('Checking if there have been any changes in the download parameters. New final dataset will be created again if required...')\n",
    "    existing_prices_TI_sentiment_scores_file = glob.glob(session_details['prices_TI_sentiment_scores_path'] + '*')\n",
    "    if (not existing_prices_TI_sentiment_scores_file) or session_details['changes']:\n",
    "        if existing_prices_TI_sentiment_scores_file:\n",
    "            os.remove(existing_prices_TI_sentiment_scores_file[0])\n",
    "            print('Deleted existing final dataset.')\n",
    "        # Build new final dataset filename\n",
    "        ticker = download_params['ticker'] + '_'\n",
    "        since = download_params['since'].replace('-', '') + '_'\n",
    "        until = download_params['until'].replace('-', '') + '_'\n",
    "        new_prices_TI_sentiment_scores_filename = ticker + since + until + 'prices_TI_ss.csv'\n",
    "        new_prices_TI_sentiment_scores_path = session_details['prices_TI_sentiment_scores_path'] + new_prices_TI_sentiment_scores_filename\n",
    "        prices_TI_file_path = prices_TI_path + session_details['prices_TI_file']\n",
    "        tweets_sentiment_scores_file_path = session_details['tweets_sentiment_scores_path'] + session_details['tweets_sentiment_scores_file']\n",
    "        df_prices_TI = pd.read_csv(prices_TI_file_path, engine='python', index_col=[0])\n",
    "        df_tweets_sentiment_scores = pd.read_csv(tweets_sentiment_scores_file_path, engine='python', index_col=[0])\n",
    "        df_prices_TI_sentiment_scores = df_prices_TI.join(df_tweets_sentiment_scores, how='left')\n",
    "        df_prices_TI_sentiment_scores.to_csv(new_prices_TI_sentiment_scores_path)\n",
    "        print('Completed joining new final dataset.')\n",
    "        print(f'File saved in \"prices_TI_sentiment_scores\" folder as {new_prices_TI_sentiment_scores_filename}.')\n",
    "    \n",
    "    elif existing_prices_TI_sentiment_scores_file and (not session_details['changes']):\n",
    "        print('Relevant final dataset file already exists in \"prices_TI_sentiment_scores\" folder. No neeed to join again.')\n",
    "    \n",
    "    new_session_details['prices_TI_sentiment_scores_file'] = glob.glob(prices_TI_sentiment_scores_path + '*')[0].replace(prices_TI_sentiment_scores_path, '')\n",
    "    new_session_details['prices_TI_sentiment_scores_full_path'] = new_session_details['prices_TI_sentiment_scores_path'] + \\\n",
    "                                                                   new_session_details['prices_TI_sentiment_scores_file']\n",
    "    \n",
    "    return new_session_details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backtesting the trading strategy with signals produced by the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now the final dataset including all the historical prices, the technical indicators and the tweets sentiment scores. Therefore we can now proceed to provide the labels for the supervised machine learning model, which will be buy and sell signals based on simple conditions on the daily prices. Then we will train a random forest model and finally we will test it and produce some performance statistics.\n",
    "\n",
    "To summarise the next steps:\n",
    "* Computing the trading strategy signals\n",
    "* Instantiating and training the random forest model\n",
    "* Testing the random forest model and producing the strategy performance statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before building the actual class for the startegy backtest, let's build some utility functions which will help us to achieve an interacting way for the user to optimise and train the model, further to managing the files associated with the model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def manage_model_training_param_folders():\n",
    "    \"\"\"\n",
    "    Check if the root directory and subdirectories for storing the model optimisation\n",
    "    and training parameters data exist.\n",
    "    Create them if they don't exist.\n",
    "    Return subdirectories paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    root = 'data/model_training_param/'\n",
    "    subdirectories = [root + sub for sub in['best_params/', 'param_grids/']]\n",
    "    \n",
    "    # header\n",
    "    header = '| MODEL PARAMETERS FOLDERS CHECK |'\n",
    "    print('\\n')\n",
    "    print('-'*len(header))\n",
    "    print(header)\n",
    "    print('-'*len(header))\n",
    "    \n",
    "    # Check if root exists, create it if it doesn't exist already\n",
    "    if os.path.isdir(root):\n",
    "        print('\\nThe \"model_training_param\" directory already exists. Checking for existing files...')\n",
    "    else:\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        print('\\nThe \"model_training_param\" directory was created. Checking for existing files...')\n",
    "    \n",
    "    # Check if subdirectories exist, create them if they don't exist already\n",
    "    for sub in subdirectories:\n",
    "        if not os.path.isdir(sub):\n",
    "            os.makedirs(sub, exist_ok=True)\n",
    "    \n",
    "    return (subdirectories[0], subdirectories[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_model_training_default_param(subdirectories):\n",
    "    \"\"\"\n",
    "    Check if 'default_best_param.json' and 'default_param_grid.json' files exist.\n",
    "    If not, create and save them in their folders.\n",
    "    Return the both subdirectories containing the files.\n",
    "    \n",
    "    Arguments:\n",
    "        subs: tuple, subdirectories full paths\n",
    "    \"\"\"\n",
    "    \n",
    "    # Instantiate default parameters for RandomForestClassifier as per sklearn documentation\n",
    "    default_best_param = {'bootstrap': True,\n",
    "                          'criterion' : 'gini',\n",
    "                          'max_depth': None,\n",
    "                          'max_features': 'auto',\n",
    "                          'min_samples_leaf': 1,\n",
    "                          'min_samples_split': 2,\n",
    "                          'n_estimators': 100}\n",
    "    \n",
    "    default_param_grid = {'bootstrap': [True],\n",
    "                          'criterion' : ['gini'],\n",
    "                          'max_depth': [None],\n",
    "                          'max_features': ['auto'],\n",
    "                          'min_samples_leaf': [1],\n",
    "                          'min_samples_split': [2],\n",
    "                          'n_estimators': [100]}\n",
    "    \n",
    "    filenames = ('default_best_param', 'default_param_grid')\n",
    "    default_params = (default_best_param, default_param_grid)\n",
    "    \n",
    "    # Save as json files the dictionaries of parameters\n",
    "    for i in range(len(subdirectories)):\n",
    "        filename_full_path = subdirectories[i] + filenames[i] + '.json'\n",
    "        if not os.path.isfile(filename_full_path):\n",
    "            with open(filename_full_path, 'w') as f:\n",
    "                json.dump(default_params[i], f)\n",
    "            print(f'\\n- \"{filenames[i]}\" file creted and saved.')\n",
    "        elif os.path.isfile(filename_full_path):\n",
    "            print(f'\\n- \"{filenames[i]}\" file already existing.')\n",
    "        \n",
    "    return subdirectories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_param(subdirectory):\n",
    "    \"\"\"\n",
    "    Scan the directory passed and if multiple files existing,\n",
    "    prompt to select a file to load.\n",
    "    Retun file selected, or return the unique file in the directory.\n",
    "    \n",
    "    Arguments:\n",
    "        subdirectory: str, full path to directory to scan\n",
    "    \"\"\"\n",
    "    \n",
    "    existing_files = glob.glob(subdirectory + '*')\n",
    "    description = subdirectory.split('/')[2]\n",
    "    if description == 'best_params':\n",
    "        file_description = 'Model Parameters'\n",
    "    if description == 'param_grids':\n",
    "        file_description = 'Grid Search Parameters'\n",
    "    \n",
    "    # Handle 1 file existing case\n",
    "    if len(existing_files) == 1:\n",
    "        file_full_path = existing_files[0]\n",
    "        with open(file_full_path, 'r') as f:\n",
    "            selected_file = json.load(f)\n",
    "    \n",
    "    # Handle muliple files existing case\n",
    "    elif len(existing_files) > 1:\n",
    "        print('\\n' + '*'*68)\n",
    "        header = f'| {file_description.upper()} SELECTION |'\n",
    "        print('\\n')\n",
    "        print('-'*len(header))\n",
    "        print(header)\n",
    "        print('-'*len(header))\n",
    "        print('\\nGeneral instructions:')\n",
    "        print('- To select a set of parameters, enter its number and press enter.')\n",
    "        \n",
    "        loaded_files = []\n",
    "        print('\\nExisting files:\\n')\n",
    "        for i in range(len(existing_files)):\n",
    "            file_full_path = existing_files[i]\n",
    "            filename = file_full_path.split('/')[3]\n",
    "            with open(file_full_path, 'r') as f:\n",
    "                file = json.load(f)\n",
    "            print('\\n', file_description + ':', i+1)\n",
    "            print(f'\\n- Filename: {filename}\\n')\n",
    "            pprint(file)\n",
    "            print('\\n')\n",
    "            loaded_files.append(file)\n",
    "            \n",
    "        valid = False\n",
    "        while not valid:\n",
    "            selection = input('\\nEnter the parameters\\' set number you want to use: ')\n",
    "            if (not selection.isnumeric()):\n",
    "                print('\\n' + '*'*68)\n",
    "                print(f'\\nValue type Error!: {selection}')\n",
    "                print(f'\\n- The value type must be a number from the list above.\\n- You entered {selection}.')\n",
    "                continue\n",
    "            \n",
    "            else:\n",
    "                if ((eval(selection) - 1) not in range(len(existing_files))):\n",
    "                    print('\\n' + '*'*68)\n",
    "                    print(f'\\nValue type Error!: {selection}')\n",
    "                    print(f'\\n- The value type must be a number from the list above.\\n- You entered {selection}.')\n",
    "                    continue\n",
    "            \n",
    "            selection = eval(selection)\n",
    "            idx = selection - 1\n",
    "            print(f'\\n{file_description.lower().capitalize()} selected: {selection}')\n",
    "            print(f'\\nFilename: {existing_files[idx].split(\"/\")[2]}\\n')\n",
    "            pprint(loaded_files[idx])\n",
    "            \n",
    "            confirm = False\n",
    "            while not confirm:\n",
    "                answer = input('\\nDo you want to confirm your selection? [Y/n]: ')\n",
    "                if answer.lower() == 'y':\n",
    "                    selected_file = loaded_files[idx]\n",
    "                    valid = True\n",
    "                    confirm = True\n",
    "                elif answer.lower() == 'n':\n",
    "                    confirm = True\n",
    "        \n",
    "    return selected_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_with_best_param(best_params_folder_full_path, model, best_param, X_train, y_train, param_selection=True):\n",
    "    \"\"\"\n",
    "    Call \"select_param\" function if multiple best_param files exist.\n",
    "    Train the model and show accuracy result.\n",
    "    Prompt to proceed to training.\n",
    "    Return the model fit on the data with selected parameters.\n",
    "    \n",
    "    Arguments:\n",
    "        best_param_folder_full_path: str, full path to folder containing the best_param files\n",
    "        model: instance of RandomForestClassifier\n",
    "        best_param: dict, dictionary of model best parameters returned by \"run_param_grid_search\" function \n",
    "        X_train: pandas dataframe, features for training\n",
    "        y_train: pandas dataframe, labels for training\n",
    "        param_selection: bool, whether to skip the best_param file selection\n",
    "    \"\"\"\n",
    "    \n",
    "    proceed = False\n",
    "    while not proceed:\n",
    "        if param_selection:\n",
    "            selected_best_param = select_param(best_params_folder_full_path)\n",
    "        elif not param_selection:\n",
    "            selected_best_param = best_param\n",
    "        RFC_model = model\n",
    "        RFC_model.set_params(**selected_best_param)\n",
    "        RFC_model.fit(X_train, y_train)\n",
    "        accuracy = RFC_model.score(X_train, y_train)\n",
    "        \n",
    "        # Display results\n",
    "        print('\\n' + '*'*68)\n",
    "        header = '| MODEL TRAINING SUMMARY |'\n",
    "        print('\\n')\n",
    "        print('-'*len(header))\n",
    "        print(header)\n",
    "        print('-'*len(header))\n",
    "        print('\\nParameters used:')\n",
    "        pprint(selected_best_param)\n",
    "        print(f'\\nModel accuracy:\\n- {round(accuracy*100,1)}%')\n",
    "        \n",
    "        # Confirm results and model to proceed to model testing\n",
    "        confirm = False\n",
    "        while not confirm:\n",
    "            answer = input('\\nDo you wish to confirm the model and complete the training session? [Y/n]: ')\n",
    "            if answer.lower() == 'y':\n",
    "                selected_best_param = save_delete_files(best_params_folder_full_path, selected_best_param)\n",
    "                confirm = True\n",
    "                proceed = True\n",
    "            \n",
    "            elif answer.lower() == 'n':    \n",
    "                repeat_or_exit = False\n",
    "                while not repeat_or_exit:\n",
    "                    answer = input('\\nDo you wish to repeat the training process or to exit the program? [R/e]: ')\n",
    "                    if answer.lower() == 'r':\n",
    "                        selected_best_param = save_delete_files(best_params_folder_full_path, selected_best_param)\n",
    "                        repeat_or_exit = True\n",
    "                        confirm = True\n",
    "                    elif answer.lower() == 'e':\n",
    "                        selected_best_param = save_delete_files(best_params_folder_full_path, selected_best_param)\n",
    "                        repeat_or_exit = True\n",
    "                        confirm = True\n",
    "                        proceed = True\n",
    "                        exit(\"Model training session terminated by user.\")\n",
    "    \n",
    "    return RFC_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_delete_files(subdirectory, param_dict, recursive_mode_on=False):\n",
    "    \"\"\"\n",
    "    Allow user to save and delete parameters files.\n",
    "    Return by the way the current parameters dictionay.\n",
    "    \n",
    "    Arguments:\n",
    "        subdirectory: str, full path to the relevant folder\n",
    "        param_dict: doct, set of parameters\n",
    "    \"\"\"\n",
    "    \n",
    "    saved = False\n",
    "    folder_name = subdirectory.split('/')[1][:-1]\n",
    "    description = subdirectory.split('/')[2]\n",
    "    if description == 'best_params':\n",
    "        file_description = 'Model Parameters'\n",
    "    if description == 'param_grids':\n",
    "        file_description = 'Grid Search Parameters'\n",
    "    \n",
    "    # File Saving session\n",
    "    print('\\n' + '*'*68)\n",
    "    header = f'| SAVE {file_description.upper()} FILES |'\n",
    "    print('\\n')\n",
    "    print('-'*len(header))\n",
    "    print(header)\n",
    "    print('-'*len(header))\n",
    "    \n",
    "    # Load up existing files, prompt if same parameters set is already saved (identify by content)\n",
    "    existing_files = glob.glob(subdirectory + '*')\n",
    "    existing_filenames = [f.split('/')[3] for f in existing_files]\n",
    "    loaded_files = []\n",
    "    if existing_files:\n",
    "        for i in range(len(existing_files)):\n",
    "            file_full_path = existing_files[i]\n",
    "            with open(file_full_path, 'r') as f:\n",
    "                file = json.load(f)\n",
    "            loaded_files.append(file)\n",
    "            \n",
    "        if param_dict in loaded_files:\n",
    "            print('\\nA file with the same paramteres already exists.')\n",
    "            saved = True\n",
    "    \n",
    "    # Handle no file in the folder case\n",
    "    # The function is called recursively when there are no files left due to deletion within the function (below)\n",
    "    if (not existing_files) and (not recursive_mode_on):\n",
    "        print('\\nAt lest one parameters set is required to proceed!')\n",
    "    \n",
    "    # Manage file saving \n",
    "    while not saved:\n",
    "        answer = input('\\nDo you want to save the file? [Y/n]: ')\n",
    "        if answer.lower() == 'n':\n",
    "            if not existing_files:\n",
    "                # Prompt to save until at least a file is in the folder\n",
    "                print('\\nAt lest one parameters set is required to proceed!')\n",
    "                continue\n",
    "            elif existing_files:\n",
    "                saved = True\n",
    "        elif answer.lower( ) == 'y':\n",
    "            if existing_files:\n",
    "                print('\\nExisting files:\\n')\n",
    "                for i in range(len(loaded_files)):\n",
    "                    print(f'\\n{file_description} {i+1}:')\n",
    "                    print(f'\\n- Filename: {existing_filenames[i]}\\n')\n",
    "                    pprint(loaded_files[i])\n",
    "                    print('\\n')\n",
    "            \n",
    "            # Manage same filename already existing\n",
    "            name = False\n",
    "            while not name:\n",
    "                filename = input('\\nProvide the filename you want to save the file with: ')\n",
    "                filename = filename + '.json'\n",
    "                if (existing_filenames) and (filename in existing_filenames):\n",
    "                    print('\\nA file with the same name already exists. It will be overwritten with the new file.')\n",
    "                print(f'\\nThe new file will be saved as: ')\n",
    "                print(f'\\nFilename: {filename}\\n')\n",
    "                pprint(param_dict)\n",
    "                \n",
    "                # Confirm saving and execute or abort\n",
    "                confirm = False\n",
    "                while not confirm:\n",
    "                    answer = input('\\nDo you want to proceed? [Y/n]: ')\n",
    "                    if answer.lower() == 'y':\n",
    "                        filename_full_path = subdirectory + filename\n",
    "                        with open(filename_full_path, 'w') as f:\n",
    "                            json.dump(param_dict, f)\n",
    "                            confirm = True\n",
    "                            name = True\n",
    "                            saved = True\n",
    "                    elif answer.lower() == 'n':\n",
    "                        confirm = True\n",
    "                        name = True\n",
    "                        continue\n",
    "    \n",
    "    # File deletion session\n",
    "    print('\\n' + '*'*68)\n",
    "    header = f'| DELETE {file_description.upper()} FILES |'\n",
    "    print('\\n')\n",
    "    print('-'*len(header))\n",
    "    print(header)\n",
    "    print('-'*len(header))\n",
    "    \n",
    "    # Load up existing files, prompt if same parameters set is already saved (identify by content)\n",
    "    existing_files = glob.glob(subdirectory + '*')\n",
    "    existing_filenames = [f.split('/')[3] for f in existing_files]\n",
    "    loaded_files = []\n",
    "    if existing_files:\n",
    "        for i in range(len(existing_files)):\n",
    "            file_full_path = existing_files[i]\n",
    "            with open(file_full_path, 'r') as f:\n",
    "                file = json.load(f)\n",
    "            loaded_files.append(file)\n",
    "    \n",
    "    if not existing_files:\n",
    "        print(f'\\nNo files saved in the {folder_name}.')\n",
    "    \n",
    "    # Manage file deletion\n",
    "    delete = False\n",
    "    while (existing_files) and not delete:\n",
    "        print('\\nExisting files:\\n')\n",
    "        for i in range(len(loaded_files)):\n",
    "            print(f'\\n{file_description} {i+1}:')\n",
    "            print(f'\\n- Filename: {existing_filenames[i]}\\n')\n",
    "            pprint(loaded_files[i])\n",
    "            print('\\n')\n",
    "        \n",
    "        # Ask for will to delete\n",
    "        confirm = False\n",
    "        while not confirm:\n",
    "            answer = input('\\nDo you want to delete files? [Y/n]: ')\n",
    "            if answer.lower() == 'y':\n",
    "                \n",
    "                # Validate the answer and execute if will to delete is confirmed\n",
    "                valid = False\n",
    "                while not valid:\n",
    "                    selection = input('\\nEnter the file number you want to delete: ')\n",
    "                    if (not selection.isnumeric()):\n",
    "                        print('\\n' + '*'*68)\n",
    "                        print(f'\\nValue type Error!: {selection}')\n",
    "                        print(f'\\n- The value type must be a number from the list above.\\n- You entered {selection}.')\n",
    "                        continue\n",
    "                    elif ((eval(selection) - 1) not in range(len(existing_files))):\n",
    "                        print('\\n' + '*'*68)\n",
    "                        print(f'\\nValue type Error!: {selection}')\n",
    "                        print(f'\\n- The value type must be a number from the list above.\\n- You entered {selection}.')\n",
    "                        continue\n",
    "                    else:\n",
    "                        selection = eval(selection)\n",
    "                        idx = selection - 1\n",
    "                        print(f'\\nThe selected files is: ')\n",
    "                        print(f'\\n{file_description} {selection}:')\n",
    "                        print(f'- Filename: {existing_filenames[idx]}\\n')\n",
    "                        pprint(loaded_files[idx])\n",
    "                        \n",
    "                        confirm_delete = False\n",
    "                        while not confirm_delete:\n",
    "                            answer = input('\\nDo you want to confirm your selection and delete the file? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                os.remove(existing_files[idx])\n",
    "                                del existing_files[idx]\n",
    "                                del existing_filenames[idx]\n",
    "                                del loaded_files[idx]\n",
    "                                # Manage case where all files have been deleted, call fuction recursively to allow saving again\n",
    "                                if not existing_files:\n",
    "                                    print('\\n' + '*'*68)\n",
    "                                    print('\\nAt lest one parameters set is required to proceed!')\n",
    "                                    save_delete_files(subdirectory, param_dict, recursive_mode_on=True)\n",
    "                                confirm_delete = True\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                                print('\\n' + '*'*68)\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm_delete = True\n",
    "                                valid = True\n",
    "                    \n",
    "            if answer.lower() == 'n':\n",
    "                confirm = True\n",
    "                delete = True\n",
    "    \n",
    "    return param_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_param_grid(selected_param_grid):\n",
    "    \"\"\"\n",
    "    Allow user to update the values in the param_grid dictionary\n",
    "    used to run the model hyperparameters optimisation with the\n",
    "    sklearn GridSearchCV.\n",
    "    \n",
    "    Arguments:\n",
    "        chosen_param_grid: dict, param_grid selected previously from \"select_param\" function\n",
    "    \"\"\"\n",
    "\n",
    "    param_grid = selected_param_grid\n",
    "    \n",
    "    print('\\n' + '*'*68)\n",
    "    header = '| UPDATING THE GRID SEARCH PARAMETERS |'\n",
    "    print('\\n')\n",
    "    print('-'*len(header))\n",
    "    print(header)\n",
    "    print('-'*len(header))\n",
    "    print('\\nGeneral instructions:')\n",
    "    print('- If you do not wish to update a parameter, do not enter anything and just press enter.')\n",
    "    print('- To enter multiple values, separate them with a space. Once you\\'re done, press enter to confirm.')\n",
    "    \n",
    "    proceed = False\n",
    "    repeat = False\n",
    "    while not proceed:\n",
    "        if not repeat:\n",
    "            answer = input('\\nDo you want to update the parameters grid? [Y/n]: ')\n",
    "        elif repeat:\n",
    "            answer = input('\\nDo you want to update the parameters grid again? [Y/n]: ')\n",
    "        if answer.lower() == 'n':\n",
    "            proceed = True\n",
    "        elif answer.lower() == 'y':\n",
    "\n",
    "            # bootstrap\n",
    "            update = False\n",
    "            print('\\nParameter: \"bootstrap\"')\n",
    "            print('- Accepted value types: bool \"True\", \"False\".')\n",
    "            print(f'- Current value: {param_grid[\"bootstrap\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"bootstrap\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"bootstrap\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if item.lower().capitalize() not in ['True', 'False', True, False]:\n",
    "                                print('\\n' + '*'*68)\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: bool \"True\", \"False\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            entry[i] = eval((entry[i].lower().capitalize()))\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"bootstrap\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['bootstrap'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "            \n",
    "            # criterion\n",
    "            update = False\n",
    "            print('\\nParameter: \"criterion\"')\n",
    "            print('- Accepted value types: \"gini\", “entropy”.')\n",
    "            print(f'- Current value: {param_grid[\"criterion\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"criterion\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"criterion\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if item.lower() not in [\"gini\", \"entropy\"]:\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: \"gini\", “entropy”.\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            entry[i] = entry[i].lower()\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"criterion\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['criterion'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "                        \n",
    "            # max_depth\n",
    "            update = False\n",
    "            print('\\nParameter: \"max_depth\"')\n",
    "            print('- Accepted value types: positive \"integer\", \"None\".')\n",
    "            print(f'- Current value: {param_grid[\"max_depth\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"max_depth\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"max_depth\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if (not item.isnumeric()) and (item.lower().capitalize() != 'None'):\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: positive \"integer\", \"None\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                        \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            if entry[i].lower().capitalize() == 'None':\n",
    "                                entry[i] = entry[i].lower().capitalize()\n",
    "                            entry[i] = eval((entry[i]))\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"max_depth\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['max_depth'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "            \n",
    "            # max_features\n",
    "            update = False\n",
    "            print('\\nParameter: \"max_features\"')\n",
    "            print('- Accepted value types: positive \"integer\", positive \"float\", \"auto\", \"sqrt\", \"log2\".')\n",
    "            print(f'- Current value: {param_grid[\"max_features\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"max_features\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"max_features\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if (not item.replace('.', '').isnumeric()) and (item.lower() not in ['auto', 'sqrt', 'log2']):\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: positive \"integer\", positive \"float\", \"auto\", \"sqrt\", \"log2\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            if entry[i].lower() not in ['auto', 'sqrt', 'log2']:\n",
    "                                entry[i] = eval((entry[i]))\n",
    "                            else:\n",
    "                                entry[i] = entry[i].lower()\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"max_features\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['max_features'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "            \n",
    "            # min_samples_leaf\n",
    "            update = False\n",
    "            print('\\nParameter: \"min_samples_leaf\"')\n",
    "            print('- Accepted value types: positive \"integer\", positive \"float\".')\n",
    "            print(f'- Current value: {param_grid[\"min_samples_leaf\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"min_samples_leaf\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"min_samples_leaf\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if not item.replace('.', '').isnumeric():\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: positive \"integer\", positive \"float\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            entry[i] = eval((entry[i]))\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"min_samples_leaf\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['min_samples_leaf'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "            \n",
    "            # min_samples_split\n",
    "            update = False\n",
    "            print('\\nParameter: \"min_samples_split\"')\n",
    "            print('- Accepted value types: positive \"integer\" greater than 1, positive \"float\".')\n",
    "            print(f'- Current value: {param_grid[\"min_samples_split\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"min_samples_split\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"min_samples_split\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if not item.replace('.', '').isnumeric():\n",
    "                                print('\\n' + '*'*68)\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: positive \"integer\", positive \"float\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            entry[i] = eval((entry[i]))\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"min_samples_split\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['min_samples_split'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "                        \n",
    "            # n_estimators\n",
    "            update = False\n",
    "            print('\\nParameter: \"n_estimators\"')\n",
    "            print('- Accepted value type: positive \"integer\".')\n",
    "            print(f'- Current value: {param_grid[\"n_estimators\"]}')\n",
    "            while not update:\n",
    "                entry = input('\\nDo you want to update \"n_estimators\"? [Y/n]: ')\n",
    "                if entry.lower() == 'n':\n",
    "                    update = True\n",
    "                \n",
    "                elif entry.lower() == 'y':\n",
    "                    update = True\n",
    "                    valid = False\n",
    "                    while not valid:\n",
    "                        error = False\n",
    "                        entry = input('\\nType your entry for \"n_estimators\" here and then press enter: ')\n",
    "                        entry = entry.split()\n",
    "                        for item in entry:\n",
    "                            if not item.isnumeric():\n",
    "                                print('\\n' + '*'*68)\n",
    "                                print(f'\\nValue type Error!: {item}')\n",
    "                                print(f'\\n- The value type must be: positive \"integer\".\\n- You entered {entry}.')\n",
    "                                error = True\n",
    "                                break\n",
    "                                \n",
    "                        if error:\n",
    "                            continue\n",
    "                    \n",
    "                        for i in range(len(entry)):\n",
    "                            entry[i] = eval((entry[i]))\n",
    "                    \n",
    "                        print(f'\\nYour new entry for \"n_estimators\": {entry}')\n",
    "                        confirm = False\n",
    "                        while not confirm:\n",
    "                            answer = input('\\nDo you want to confirm your entry? [Y/n]: ')\n",
    "                            if answer.lower() == 'y':\n",
    "                                param_grid['n_estimators'] = entry\n",
    "                                valid = True\n",
    "                                confirm = True\n",
    "                            elif answer.lower() == 'n':\n",
    "                                confirm = True\n",
    "            \n",
    "            # Prompt for further update\n",
    "            repeat = True\n",
    "    \n",
    "    return param_grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_param_grid_search(param_grids_folder_full_path, model, X_train, y_train):\n",
    "    \"\"\"\n",
    "    Call \"select_param\" function if multiple pram_grid files exist.\n",
    "    Run grid search and show accuracy result.\n",
    "    Allow to save or delete param_grid files.\n",
    "    Return the model fit on the data with the best performing parameters.\n",
    "    \n",
    "    Arguments:\n",
    "        param_grids_folder_full_path: str, full path to folder containing the param_grid files\n",
    "        model: instance of RandomForestClassifier\n",
    "        X_train: pandas dataframe, features for training\n",
    "        y_train: pandas dataframe, labels for training\n",
    "    \"\"\"\n",
    "    \n",
    "    proceed = False\n",
    "    while not proceed:\n",
    "        # Select param_grid from existing files in the param_grids folder\n",
    "        selected_param_grid = select_param(param_grids_folder_full_path)\n",
    "        \n",
    "        # Update the param_grid if wished\n",
    "        selected_param_grid = update_param_grid(selected_param_grid)\n",
    "        \n",
    "        # Compute grid search for best estimator\n",
    "        RFC_grid_search = GridSearchCV(model, param_grid=selected_param_grid, cv=5, n_jobs=-1, verbose=4)\n",
    "        RFC_grid_search.fit(X_train, y_train)\n",
    "        \n",
    "        # Retrieve model performance details\n",
    "        RFC_best_model = RFC_grid_search.best_estimator_\n",
    "        feature_importances = RFC_best_model.feature_importances_\n",
    "        sorted_idx = RFC_best_model.feature_importances_.argsort()\n",
    "        accuracy = RFC_grid_search.best_score_\n",
    "        best_param = RFC_grid_search.best_params_\n",
    "        \n",
    "        # Display results\n",
    "        print('\\n' + '*'*68)\n",
    "        header = '| MODEL HYPERPARAMETERS OPTIMISATION SUMMARY |'\n",
    "        print('\\n')\n",
    "        print('-'*len(header))\n",
    "        print(header)\n",
    "        print('-'*len(header))\n",
    "        print('\\nParameters grid used:')\n",
    "        pprint(selected_param_grid)\n",
    "        print('\\nBest model parameters:')\n",
    "        pprint(best_param)\n",
    "        print(f'\\nBest model cross-validated mean accuracy:\\n- {round(accuracy*100,1)}%')\n",
    "        \n",
    "        df = pd.DataFrame(feature_importances, index=X_train.columns).sort_values(by=0, ascending=False)\n",
    "        \n",
    "        # Some plotting settings\n",
    "        text_size = 19\n",
    "        x_ticks_size = 14\n",
    "        y_ticks_size = x_ticks_size\n",
    "        pad_title = 15\n",
    "        pad_xlabel = 20  \n",
    "        pad_ylabel = 70\n",
    "        \n",
    "        # Plot feature importances\n",
    "        sns.set_style('darkgrid')\n",
    "        sns.set_palette(sns.color_palette(\"mako\"))\n",
    "        fig, ax = plt.subplots()\n",
    "        fig.set_size_inches(10, df.shape[0]*0.4)\n",
    "        sns.barplot(x=df[0], y=df.index, data=df)\n",
    "        plt.xlabel('Importance', labelpad=pad_xlabel, size=text_size)\n",
    "        plt.xticks(fontsize=x_ticks_size)\n",
    "        plt.yticks(fontsize=y_ticks_size)\n",
    "        plt.title('Feature Importances', size=text_size*1.7, pad=pad_title)\n",
    "        plt.show()\n",
    "        \n",
    "        # Confirm results and model to proceed to model testing\n",
    "        confirm = False\n",
    "        while not confirm:\n",
    "            answer = input('\\nDo you wish to confirm the model and proceed to train the best model? [Y/n]: ')\n",
    "            if answer.lower() == 'y':\n",
    "                # Save the param_grid if wished\n",
    "                selected_param_grid = save_delete_files(param_grids_folder_full_path, selected_param_grid)\n",
    "                confirm = True\n",
    "                proceed = True\n",
    "                \n",
    "            elif answer.lower() == 'n':\n",
    "                \n",
    "                repeat_or_exit = False\n",
    "                while not repeat_or_exit:\n",
    "                    answer = input('\\nDo you wish to repeat the training process or to exit the program? [R/e]: ')\n",
    "                    if answer.lower() == 'r':\n",
    "                        selected_param_grid = save_delete_files(param_grids_folder_full_path, selected_param_grid)\n",
    "                        repeat_or_exit = True\n",
    "                        confirm = True\n",
    "                    elif answer.lower() == 'e':\n",
    "                        selected_param_grid = save_delete_files(param_grids_folder_full_path, selected_param_grid)\n",
    "                        repeat_or_exit = True\n",
    "                        confirm = True\n",
    "                        proceed = True\n",
    "                        exit(\"Model training session terminated by user.\")\n",
    "        \n",
    "    \n",
    "    return RFC_best_model, best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manage_strategy_performance_folder(try_mode_on=False):\n",
    "    \"\"\"\n",
    "    Check if the subdirectory for storing the strategy performance datset exists.\n",
    "    Create it if it don't exist.\n",
    "    Return subdirectory paths.\n",
    "    \"\"\"\n",
    "    \n",
    "    root = 'data/strategy_performance/'\n",
    "    \n",
    "    if not try_mode_on:\n",
    "        # header\n",
    "        header = '| STRATEGY PERFORMANCE FOLDERS CHECK |'\n",
    "        print('\\n')\n",
    "        print('-'*len(header))\n",
    "        print(header)\n",
    "        print('-'*len(header))\n",
    "        \n",
    "        # Check if folder exists, create it if it doesn't exist already\n",
    "        if os.path.isdir(root):\n",
    "            print('\\nThe \"strategy_performance\" folder already exists.')\n",
    "        else:\n",
    "            os.makedirs(root, exist_ok=True)\n",
    "            print('\\nThe \"strategy_performance\" folder was created.')\n",
    "    \n",
    "    return root"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Backtest_Strategy class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have all the functions needed to design the class that will allow us to preprocess the data, handling missing values and data normalisation, produce the trading signals, train, test the model, and display the strategy performance indicators.\n",
    "\n",
    "The whole training, testing and trading strategy performance utilities will be implemented in an interactive way where the user will be able to optimise the hyperparameters, visualise the ML model performance, save and delete files storing the model hyperparameters.\n",
    "\n",
    "This is to keep consistency with the level of automation of the data download process and to deliver a better user experience, where one can focus on the relevant parameters whilst the program will execute behind the scenes all the changes required, avoiding hard coding the changes themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backtest_Strategy():\n",
    "    \"\"\"\n",
    "    Initialise an instance loading all the aggregated data.\n",
    "    Provide methods to preprocess data, produce trading signals, train and test the ML model,\n",
    "    display the trading strategy performance.\n",
    "    \n",
    "    Methods:\n",
    "        preprocess_data: monitor and handle missing values, outliers, and mornalise data\n",
    "        compute_signals: produce the trading strategy buy and sell signals\n",
    "        train_model: train the ML model, optimise the hyperparameters with the parameters grid search, display model proformance\n",
    "        test_model: test the model with the selected parameters, display trained model performance on unseen data\n",
    "        strategy_performance: display the trading strategy performance indicators, plot P&L and drawdown graphs\n",
    "    \"\"\"\n",
    "        \n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialise the Backtest_Strategy object.\n",
    "        Arguments:\n",
    "            data: string, full path to dataset\n",
    "        \"\"\"\n",
    "        \n",
    "        self.data_path = data_path\n",
    "\n",
    "\n",
    "    def preprocess_data(self, display_summary=True, return_df=False):\n",
    "        \"\"\"\n",
    "        Handle missing values for sentiment scores, any columns with all null values\n",
    "        and the longest rolling indicator (200 MA).\n",
    "        Display data preprocessing results summary upon request.\n",
    "        Return dataframe with preprocessed data\n",
    "        \n",
    "        Arguments:\n",
    "            display_summary: bool, whether displaying the data preparation summary\n",
    "        \"\"\"\n",
    "        \n",
    "        df = pd.read_csv(self.data_path, engine='python', index_col=[0])\n",
    "        features_start = list(df.columns).index('Volume') + 1 \n",
    "        \n",
    "        # Handle sentiment scores missing values\n",
    "        # Handle negative, neutral and positive missing scores (replacing with equal weight giving total sum=1)\n",
    "        sentiment_cols = ['Tweets_Neg_Score', 'Tweets_Neu_Score', 'Tweets_Pos_Score']\n",
    "        NNP_replace_with = {'Tweets_Neg_Score' : 0.33, 'Tweets_Neu_Score' : 0.34, 'Tweets_Pos_Score' : 0.33}\n",
    "        df.loc[:, sentiment_cols] = df.loc[:, sentiment_cols].fillna(value=NNP_replace_with)\n",
    "        \n",
    "        # Handle cumulative scores missing values (replacing with neutral value 0)\n",
    "        df.loc[:, 'Tweets_Com_Score'] = df.loc[:, 'Tweets_Com_Score'].fillna(value=0)\n",
    "        \n",
    "        # Handle infinite values, replace them with max value of the column\n",
    "        df_inf_mask = df == np.inf\n",
    "        df_no_inf_mask = df != np.inf\n",
    "        df_max = df[df_no_inf_mask].max()\n",
    "        df[df_inf_mask] = df.replace(np.inf, df_max)\n",
    "        \n",
    "        # Handle columns with only missing values\n",
    "        df = df.dropna(axis=1, how='all')\n",
    "        \n",
    "        # Handle longest rolling indicator (200 MA)\n",
    "        df = df.iloc[199:]\n",
    "        \n",
    "        # Identify potential missing values left\n",
    "        missing_values = df.isna().sum().to_frame().rename(columns={0:'Missing Values'})\n",
    "        missing_values = missing_values[missing_values['Missing Values'] != 0].sort_values('Missing Values', ascending=False)\n",
    "        \n",
    "        # Identify potential infinite values left\n",
    "        inf_values = (df == np.inf).sum().to_frame().rename(columns={0:'Infinite Values'})\n",
    "        inf_values = inf_values[inf_values['Infinite Values'] != 0].sort_values('Infinite Values', ascending=False)\n",
    "        \n",
    "        # Display missing values summary if display_summary is True\n",
    "        if display_summary:\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "                # Header\n",
    "                header = '| DATA PREPROCESSING SUMMARY |'\n",
    "                print('-'*len(header))\n",
    "                print(header)\n",
    "                print('-'*len(header))\n",
    "                \n",
    "                # Display summary of missing values\n",
    "                print('\\nMissing Values:')\n",
    "                if missing_values.shape[0] == 0:\n",
    "                    print(\"- No missing values identified\\n\")\n",
    "                elif missing_values.shape[0] != 0:\n",
    "                    display(missing_values)\n",
    "                print('*'*68)\n",
    "                \n",
    "                # Display summary of infinite values\n",
    "                print('\\nInfinite Values:')\n",
    "                if inf_values.shape[0] == 0:\n",
    "                    print(\"- No infinite values identified\\n\")\n",
    "                elif inf_values.shape[0] != 0:\n",
    "                    display(inf_values)\n",
    "                print('*'*68)\n",
    "                \n",
    "        # Features scaling if data is ok\n",
    "        if (missing_values.shape[0] == 0) and (inf_values.shape[0] == 0):\n",
    "            min_max_scaler = MinMaxScaler()\n",
    "            df.iloc[:, features_start:] = min_max_scaler.fit_transform(df.iloc[:, features_start:])\n",
    "        \n",
    "        if display_summary:\n",
    "            with pd.option_context('display.max_rows', None, 'display.max_columns', None):\n",
    "                # Display columns statistics\n",
    "                print('\\nColumns Statistics:')\n",
    "                print('- Rows from top represent: Count, Mean, Std, Min, 25%, 50%, 75%, Max')\n",
    "                display(df.describe())\n",
    "        \n",
    "        # Prompt to sort missing and infinite value and exit if identified\n",
    "        if (missing_values.shape[0] != 0) and (inf_values.shape[0] != 0):\n",
    "            print('\\nMissing and infinite values identified. Make sure these are addressed before proceeding to training the model.')\n",
    "            exit()\n",
    "        \n",
    "        elif missing_values.shape[0] != 0:\n",
    "            print('\\nMissing values identified. Make sure these are addressed before proceeding to training the model.')\n",
    "            exit()\n",
    "        \n",
    "        elif inf_values.shape[0] != 0:\n",
    "            print('\\nInfinite values identified. Make sure these are addressed before proceeding to training the model.')\n",
    "            exit()\n",
    "                \n",
    "        if return_df:\n",
    "            return df\n",
    "        \n",
    "\n",
    "    def compute_signals(self):\n",
    "        \"\"\"\n",
    "        Implement the trading startegy. Buy when the previous day price is lower,\n",
    "        short sell when the previous day is higher.\n",
    "        \n",
    "        Signals will be the output labels to train the model.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Compute trading signals\n",
    "        df = self.preprocess_data(display_summary=False, return_df=True)\n",
    "        df['Signals'] = np.nan\n",
    "        df['Signals'] = np.where(df['Close'] > df['Close'].shift(1), 1, 0)\n",
    "        df['Signals'] = np.where(df['Close'] < df['Close'].shift(1), -1, df['Signals'])\n",
    "        \n",
    "        df = df.iloc[1:] #First row has no signal\n",
    "        \n",
    "        return df\n",
    "\n",
    "        \n",
    "    def train_model(self, optimise_hyperparameters=True, train_only=True):\n",
    "        \"\"\"\n",
    "        Run parameters grid search for best performing estimator and model training.\n",
    "        \n",
    "        Option to perform the param_grid optimisation (enabled by default if\n",
    "        the method is explicitly called)\n",
    "        \n",
    "        Stop to the training process if the method is called on a Backtest_Strategy object.\n",
    "        \n",
    "        Return the model fit on training data with selected parameters\n",
    "        when called by the \"train_model\" method.\n",
    "        \n",
    "        Arguments:\n",
    "            optimise_hyperparameters: bool, enable parameters grid optimisation\n",
    "            train_only: bool, stop at the end of training, do not return anything\n",
    "        \"\"\"\n",
    "        \n",
    "        df = self.compute_signals()\n",
    "        \n",
    "        # Split points for test/train datasets\n",
    "        tot_rows = df.shape[0]\n",
    "        split_point = int(tot_rows * 0.8)\n",
    "        features_start = list(df.columns).index('Volume') + 1 \n",
    "        features_end = -1\n",
    "        labels = -1\n",
    "        \n",
    "        # train datasets\n",
    "        X_train = df.iloc[:split_point, features_start:features_end]\n",
    "        y_train = df.iloc[:split_point, labels]\n",
    "        \n",
    "        # Check for best_param and param_grid folders and files, get folders paths\n",
    "        sub_directories = manage_model_training_param_folders()\n",
    "        sub_directories = create_model_training_default_param(sub_directories)\n",
    "        best_params_folder_full_path = sub_directories[0]\n",
    "        param_grids_folder_full_path = sub_directories[1]\n",
    "        \n",
    "        # Instantiate model \n",
    "        RFC_model = RandomForestClassifier()\n",
    "        \n",
    "        # Opt for running the grid search\n",
    "        if optimise_hyperparameters:\n",
    "            use_param_grid_search = False\n",
    "            while not use_param_grid_search:\n",
    "                print('\\n' + '*'*68)\n",
    "                answer = input('\\nDo you wish to optimise the hyperparameters with the grid search? [Y/n]: ')\n",
    "                if answer.lower() == 'y':\n",
    "                    optimise_hyperparameters = True\n",
    "                    use_param_grid_search = True\n",
    "                elif answer.lower() == 'n':\n",
    "                    optimise_hyperparameters = False\n",
    "                    use_param_grid_search = True\n",
    "                \n",
    "        # Run this by default when train_model method is called explicitly from a Backtest_Strategy instance\n",
    "        # Allow for definition of grid search parameters\n",
    "        best_param = None\n",
    "        param_selection = True\n",
    "        if optimise_hyperparameters:\n",
    "            param_selection = False\n",
    "            RFC_model, best_param = run_param_grid_search(param_grids_folder_full_path, RFC_model, X_train, y_train)\n",
    "        \n",
    "        # Run with one of the available best_param files in the best_params folder after selection\n",
    "        # Default mode when calling the test_model or strategy_performance methods\n",
    "        RFC_model = train_with_best_param(best_params_folder_full_path, RFC_model, best_param, X_train, y_train, param_selection=param_selection)\n",
    "        \n",
    "        if not train_only:\n",
    "            return RFC_model, split_point, features_start, features_end, labels\n",
    "\n",
    "\n",
    "    def test_model(self):\n",
    "        \"\"\"\n",
    "        Call \"train_model\" method to allow the selection of best parameters\n",
    "        to test or train the model again and save news sets of parameters.\n",
    "        Allow then for proceeding to saving the dataset with the predicted\n",
    "        signals and finally be able to display the performance summary.\n",
    "        \"\"\"\n",
    "        \n",
    "        proceed = False\n",
    "        while not proceed:\n",
    "            \n",
    "            # Get all parameters needed from train_model method\n",
    "            RFC_model, split_point, features_start, features_end, labels = self.train_model(optimise_hyperparameters=False, train_only=False)\n",
    "            \n",
    "            # Get dataframe for testing\n",
    "            df = self.compute_signals()\n",
    "            \n",
    "            # test dataset\n",
    "            X_test = df.iloc[split_point:, features_start:features_end]\n",
    "            y_test = df.iloc[split_point:, labels]\n",
    "            \n",
    "            # Retrieving testing scores\n",
    "            accuracy = RFC_model.score(X_test, y_test)\n",
    "            y_pred = RFC_model.predict(X_test)\n",
    "            precision = metrics.precision_score(y_test, y_pred)\n",
    "            recall = metrics.recall_score(y_test, y_pred)\n",
    "            tn, fp, fn, tp = metrics.confusion_matrix(y_test, y_pred).ravel()\n",
    "            specificity = tn / (tn + fp)\n",
    "            \n",
    "            # Build multi index data as dictionary for testing score summary display\n",
    "            scores = {('Testing Score Summary', 'Accuracy (%)') : [accuracy],\n",
    "                      ('Testing Score Summary', 'Precision (%)') : [precision],\n",
    "                      ('Testing Score Summary', 'Recall (%)') : [recall],\n",
    "                      ('Testing Score Summary', 'Specificity (%)') : [specificity]}\n",
    "\n",
    "            # Initialise dataframe\n",
    "            score_summary = pd.DataFrame.from_dict(scores).rename(index={0:'Scores'})\n",
    "            \n",
    "            # Display testing results\n",
    "            print('\\n' + '*'*68)\n",
    "            header = '| MODEL TESTING SUMMARY |'\n",
    "            print('\\n')\n",
    "            print('-'*len(header))\n",
    "            print(header)\n",
    "            print('-'*len(header))\n",
    "            print('\\nParameters used:')\n",
    "            pprint(RFC_model.get_params())\n",
    "            display(score_summary)\n",
    "            \n",
    "            # Some plotting settings\n",
    "            text_size = 19\n",
    "            x_ticks_size = 14\n",
    "            y_ticks_size = x_ticks_size\n",
    "            pad_title = 15\n",
    "            pad_xlabel = 20  \n",
    "            pad_ylabel = 90\n",
    "            \n",
    "            # Plot ROC curve\n",
    "            fpr, tpr, thresholds = metrics.roc_curve(y_test, y_pred)\n",
    "            roc_auc = metrics.auc(fpr, tpr)\n",
    "            roc_plot = metrics.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc, estimator_name='Random Forest Classifier')\n",
    "            sns.set_style('darkgrid')\n",
    "            sns.set_palette(sns.color_palette(\"RdBu\", 10))\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_size_inches(11.7, 8.3)\n",
    "            roc_plot.plot(ax=ax)\n",
    "            plt.xlabel('False Positive Rate', labelpad=pad_xlabel, size=text_size)\n",
    "            plt.ylabel('True Positive Rate', rotation=0, labelpad=pad_ylabel, size=text_size)\n",
    "            plt.xticks(fontsize=x_ticks_size)\n",
    "            plt.yticks(fontsize=y_ticks_size)\n",
    "            plt.title('ROC Curve', size=text_size*1.7, pad=pad_title)\n",
    "            ax.legend(loc='lower right', fontsize=x_ticks_size, frameon=False)\n",
    "            plt.show()\n",
    "            \n",
    "            # Prompt for repeating the training or go ahead with the strategy performance display\n",
    "            answer = input('\\nDo you want to repeat the model training? [Y/n]: ')\n",
    "            if answer.lower() == 'y':\n",
    "                continue\n",
    "            \n",
    "            elif answer.lower() == 'n':    \n",
    "                display_strategy_performance = False\n",
    "                while not display_strategy_performance:\n",
    "                    answer = input('\\nDo you want to go ahead and print the strategy performance statistics? [Y/n]: ')\n",
    "                    if answer.lower() == 'y':\n",
    "                        display_strategy_performance = True\n",
    "                        proceed = True\n",
    "                        \n",
    "                        # Create path to save strategy performance dataset\n",
    "                        strategy_performance_folder_path = manage_strategy_performance_folder()\n",
    "                        DF_strategy_performance_full_path = strategy_performance_folder_path + 'DF_strategy_performance.csv'\n",
    "                        \n",
    "                        # Prepare dataset for strategy performance computation\n",
    "                        df_strategy_performance = df.iloc[split_point:, :features_start]\n",
    "                        predictions = RFC_model.predict(X_test)\n",
    "                        df_strategy_performance['Signals'] = predictions\n",
    "                        \n",
    "                        # Save strategy performance dataset\n",
    "                        df_strategy_performance.to_csv(DF_strategy_performance_full_path)\n",
    "                        \n",
    "                        # Display strategy performance\n",
    "                        self.strategy_performance()\n",
    "                        \n",
    "                    elif answer.lower() == 'n':\n",
    "                        display_strategy_performance = True\n",
    "                        proceed = True\n",
    "                        exit(\"Model testing session terminated by user.\")\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "    def strategy_performance(self):\n",
    "        \"\"\"\n",
    "        Call \"test_model\" method if no dataframe with predicted signals was ever produced\n",
    "        Once test is terminated and a dataframe with predicted signals is produced, compute\n",
    "        all perfromances indicators and display a performance summary along with plotting of buy and hold\n",
    "        and trading strategy returns, and max drawdown.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Check wheteher strategy performances data was created and saved\n",
    "        strategy_performance_folder_path = manage_strategy_performance_folder(try_mode_on=True)\n",
    "        DF_strategy_performance_full_path = strategy_performance_folder_path + 'DF_strategy_performance.csv'\n",
    "        \n",
    "        # Avoid recursive call in \"test_model\" to print the performance summary twice \n",
    "        train_test_model = False\n",
    "        \n",
    "        if (not glob.glob(strategy_performance_folder_path + '*')) or (not os.path.exists(DF_strategy_performance_full_path)):\n",
    "            print('\\nStrategy performance data have not been created yet!\\n')\n",
    "            print('\\nYou will need to complete the model training and testing process at least once to display the strategy performance statistics.')\n",
    "            \n",
    "            # Prompt to train and test or exit\n",
    "            train_test_model = False\n",
    "            while not train_test_model:\n",
    "                answer = input('\\nDo you want to go ahead with the model training and testing? [Y/n]: ')\n",
    "                if answer.lower() == 'y':\n",
    "                    train_test_model = True\n",
    "                    self.test_model()\n",
    "                elif answer.lower() == 'n':\n",
    "                    train_test_model = True\n",
    "                    exit(\"Program terminated by user.\")\n",
    "        \n",
    "        # Avoid recursive call in \"test_model\" to print the performance summary twice \n",
    "        if not train_test_model:\n",
    "            \n",
    "            # Get dataframe and compute all perfomances statistics\n",
    "            df = pd.read_csv(DF_strategy_performance_full_path, index_col=[0], parse_dates=True)\n",
    "            \n",
    "            # Compute daily log returns\n",
    "            df['Returns_Daily_Log'] = np.log(df['Close']/df['Close'].shift(1))\n",
    "            \n",
    "            # Compute buy and hold returns and strategy returns\n",
    "            df['Returns_Strategy_Daily'] = df['Returns_Daily_Log'] * df['Signals']\n",
    "            df['Returns_B&H_Cum'] = df['Returns_Daily_Log'].cumsum()\n",
    "            df['Returns_Strategy_Cum'] = df['Returns_Strategy_Daily'].cumsum()\n",
    "            \n",
    "            # Compute maximum drawdown\n",
    "            df['Returns_Strategy_Max_Cum'] = df['Returns_Strategy_Cum'].cummax()\n",
    "            df['Drawdown'] = df['Returns_Strategy_Cum'] - df['Returns_Strategy_Max_Cum']\n",
    "            \n",
    "            # Compute performance statistics\n",
    "            # Some data required for the performance indicators\n",
    "            years = round((df.index[-1] - df.index[0]).days/365.25, 2)\n",
    "            risk_free_rate = ((1 + 0.065)**(1/365))-1\n",
    "            returns_strategy_daily = df['Returns_Strategy_Daily']\n",
    "            n_positive_trades = (df['Returns_Strategy_Daily'] >= 0).sum()\n",
    "            n_negative_trades = (df['Returns_Strategy_Daily'] < 0).sum()\n",
    "            n_trades = df.shape[0] - 1\n",
    "            \n",
    "            # Buy and hold returns\n",
    "            returns_buy_hold = round(df['Returns_B&H_Cum'][-1]*100, 2)\n",
    "            returns_buy_hold_ann = round((((1 + df['Returns_B&H_Cum'][-1])**(1/years))-1)*100, 2)\n",
    "            \n",
    "            # Strategy returns\n",
    "            returns_strategy = round(df['Returns_Strategy_Cum'][-1]*100, 2)\n",
    "            returns_strategy_ann = round((((1 + df['Returns_Strategy_Cum'][-1])**(1/years))-1)*100, 2)\n",
    "            \n",
    "            # Sharpe ratio\n",
    "            sharpe_ratio = round(((returns_strategy_daily - risk_free_rate).mean()/np.std((returns_strategy_daily - risk_free_rate), ddof=1)), 2)\n",
    "            \n",
    "            # Hit ratio, average trades profit, average trades loss \n",
    "            hit_ratio = round(n_positive_trades/n_trades, 2)*100\n",
    "            average_trades_profit = round(df['Returns_Strategy_Daily'][df['Returns_Strategy_Daily'] >= 0].mean()*100, 2)\n",
    "            average_trades_loss = round(df['Returns_Strategy_Daily'][df['Returns_Strategy_Daily'] < 0].mean()*100, 2)\n",
    "            \n",
    "            # Max drawdown and days to recover from max drawdown\n",
    "            max_drawdown_pct = round(df['Drawdown'].min()*100, 2)\n",
    "            df['Days_Drawdown_Recovery'] = 0\n",
    "            c = 1\n",
    "            trigger = False\n",
    "            max_drawdown = df['Drawdown'].min()\n",
    "            days_max_drawdown_recovery = 0\n",
    "            for index, row in df.iterrows():\n",
    "                if row['Drawdown'] < 0:\n",
    "                    df.at[index, 'Days_Drawdown_Recovery'] = c\n",
    "                    c += 1\n",
    "                elif row['Drawdown'] >= 0:\n",
    "                    c = 1\n",
    "                    trigger = False\n",
    "                if row['Drawdown'] == max_drawdown:\n",
    "                    trigger = True\n",
    "                    days_max_drawdown_recovery = df.loc[index, 'Days_Drawdown_Recovery']\n",
    "                    continue\n",
    "                if trigger and (row['Drawdown'] < 0):\n",
    "                    days_max_drawdown_recovery = df.loc[index, 'Days_Drawdown_Recovery']\n",
    "            if trigger:\n",
    "                days_max_drawdown_recovery = 'Never recovered'\n",
    "    \n",
    "            # Performance indicators summary dataframe\n",
    "            performance_summary_dict = {'Return Buy and Hold (%)' : str(returns_buy_hold),\n",
    "                                           'Return Buy and Hold Ann. (%)' : returns_buy_hold_ann,\n",
    "                                           'Return Trading Strategy (%)' : returns_strategy,\n",
    "                                           'Return Trading Strategy Ann. (%)' : returns_strategy_ann,\n",
    "                                           'Sharpe Ratio' : sharpe_ratio,\n",
    "                                           'Hit Ratio (%)' : hit_ratio,\n",
    "                                           'Average Trades Profit (%)' : average_trades_profit,\n",
    "                                           'Average Trades Loss (%)' : average_trades_loss,\n",
    "                                           'Max Drawdown (%)' : max_drawdown_pct,\n",
    "                                           'Days Max Drawdown Recovery': days_max_drawdown_recovery}\n",
    "            \n",
    "            performance_summary = pd.DataFrame.from_dict(performance_summary_dict, orient='index').rename(columns={0:'Performance Indicators Summary'})\n",
    "            \n",
    "            # header\n",
    "            print('\\n' + '*'*68)\n",
    "            header = '| TRADING STRATEGY PERFORMANCE |'\n",
    "            print('\\n')\n",
    "            print('-'*len(header))\n",
    "            print(header)\n",
    "            print('-'*len(header))\n",
    "            \n",
    "            # Print performance indicators summary\n",
    "            print('\\n')\n",
    "            display(performance_summary)\n",
    "            \n",
    "            # Some plotting settings\n",
    "            text_size = 19\n",
    "            x_ticks_size = 14\n",
    "            y_ticks_size = x_ticks_size\n",
    "            pad_title = 15\n",
    "            pad_xlabel = 20  \n",
    "            pad_ylabel = 70\n",
    "            \n",
    "            # Adjust drawdown and returns values\n",
    "            df['Drawdown'] = df['Drawdown'] * 100\n",
    "            df['Returns_B&H_Cum'] = df['Returns_B&H_Cum'] * 100\n",
    "            df['Returns_Strategy_Cum'] = df['Returns_Strategy_Cum'] * 100\n",
    "            \n",
    "            # Plot drawdown\n",
    "            sns.set_style('darkgrid')\n",
    "            sns.set_palette(sns.color_palette(\"RdBu\", 10))\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_size_inches(11.7, 8.3)\n",
    "            sns.lineplot(x=df.index, y='Drawdown', data=df)\n",
    "            plt.xlabel('Date', labelpad=pad_xlabel, size=text_size)\n",
    "            plt.ylabel('Drawdown (%)', rotation=0, labelpad=pad_ylabel, size=text_size)\n",
    "            plt.xticks(fontsize=x_ticks_size)\n",
    "            plt.yticks(fontsize=y_ticks_size)\n",
    "            plt.title(f'Drawdown - Trading Strategy - {download_params[\"ticker\"]}', size=text_size*1.7, pad=pad_title)\n",
    "            \n",
    "            #Plot buy annd hold and strategy returns\n",
    "            sns.set_style('darkgrid')\n",
    "            sns.set_palette(sns.color_palette(\"Dark2\"))\n",
    "            fig, ax = plt.subplots()\n",
    "            fig.set_size_inches(11.7, 8.3)\n",
    "            sns.lineplot(x=df.index, y='Returns_B&H_Cum', data=df)\n",
    "            sns.lineplot(x=df.index, y='Returns_Strategy_Cum', data=df)\n",
    "            plt.xlabel('Date', labelpad=pad_xlabel, size=text_size)\n",
    "            plt.ylabel('Return (%)', rotation=0, labelpad=pad_ylabel, size=text_size)\n",
    "            plt.xticks(fontsize=x_ticks_size)\n",
    "            plt.yticks(fontsize=y_ticks_size)\n",
    "            plt.title(f'Returns - Buy And Hold VS Trading Strategy - {download_params[\"ticker\"]}', size=text_size*1.7, pad=pad_title)\n",
    "            ax.legend(labels=['Buy And Hold', 'Trading Strategy'], loc='upper left', fontsize=x_ticks_size, frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Executing the program and backtesting the trading strategy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all in place to proceed to download and manipulate the data, to train and test the model and eventually evaluate the performance of the trading strategy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The download parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we only need to interface the data download pipeline with the download parameters.\n",
    "\n",
    "Let's declare our download parameters for this strategy backtest session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "download_params = {'ticker' : 'TSLA',\n",
    "                   'since' : '2010-06-29', \n",
    "                   'until' : '2021-06-02',\n",
    "                   'twitter_scrape_by_account' : {'elonmusk': {'search_keyword' : '',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'tesla': {'search_keyword' : '',\n",
    "                                                            'by_hashtag' : False},\n",
    "                                                  'WSJ' : {'search_keyword' : 'Tesla',\n",
    "                                                           'by_hashtag' : False},\n",
    "                                                  'Reuters' : {'search_keyword' : 'Tesla',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'business': {'search_keyword' : 'Tesla',\n",
    "                                                               'by_hashtag' : False},\n",
    "                                                  'CNBC': {'search_keyword' : 'Tesla',\n",
    "                                                           'by_hashtag' : False},\n",
    "                                                  'FinancialTimes' : {'search_keyword' : 'Tesla',\n",
    "                                                                      'by_hashtag' : True}},\n",
    "                   'twitter_scrape_by_most_popular' : {'all_twitter': {'search_keyword' : 'Tesla',\n",
    "                                                                       'max_tweets_per_day' : 30,\n",
    "                                                                       'by_hashtag' : True}},\n",
    "                   'language' : 'en'                                      \n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the data download and manipulation pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to execute the pipeline, we only need to call the \"run\" method on the instance we initialised earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "TSLA_pipeline_outputs = TSLA_data_pipeline.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the random forest model and backtesting the trading strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the data for the ML model\n",
    "data = glob.glob('data/prices_TI_sentiment_scores/*')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Initialise a Backtest_Strategy class instance\n",
    "TSLA_backtest_strategy = Backtest_Strategy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following methods are somehow interconnected and called in chain if we run a method downstream in the backtesting process.\n",
    "\n",
    "The scope behind this design is to be able to focus on a particular step when calling the relevant method, such as preprocessing the data or training the model, and when calling a more downstream method the upstream methods will be called and prompted to the user if useful or needed.\n",
    "\n",
    "As an example, if we call the \"strategy_performance\" method but the model is not trained and/or the hyperparameters are not saved, the program will prompt to train the model first in order to proceed with the backtest. Or if there are still missing values in the final dataset, the \"preprocess_data\" method will throw a warning and exit the program, regardless of where the method was called from. Thus, the methods are interlaced in a sort of pipeline where recursion is leveraged if needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Prepare the data for the machine learning model and show a summary of the preprocessing results\n",
    "TSLA_backtest_strategy.preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train the machine learning model\n",
    "TSLA_backtest_strategy.train_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# test the machine learning model\n",
    "TSLA_backtest_strategy.test_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Display the trading strategy performance\n",
    "TSLA_backtest_strategy.strategy_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='References'></a>\n",
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Minna Castoe, \"Predicting Stock Market Price Direction with Uncertainty Using Quantile Regression Forest\", Uppsala University (2020)](https://www.diva-portal.org/smash/get/diva2:1503760/FULLTEXT02)\n",
    "2. [Venkata Sasank Pagolu, Kamal Nayan Reddy, Ganapati Panda, Babita Majhi. Sentiment analysis of Twitter data for predicting stock market movements, 2016 International Conference on Signal Processing, Communication, Power and Embedded System (SCOPES)](https://ieeexplore.ieee.org/abstract/document/7955659/metrics#metrics)\n",
    "3. [Random Forest](https://en.wikipedia.org/wiki/Random_forest)\n",
    "4. [Directed Acyclic Graph (DAG)](https://www.capgemini.com/gb-en/2020/10/introducing-directed-acyclic-graphs-and-their-use-cases/)\n",
    "5. [Dataquest Data Engineer course](https://www.dataquest.io/path/data-engineer/)\n",
    "6. [Idempotence](https://stackoverflow.com/questions/1077412/what-is-an-idempotent-operation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
